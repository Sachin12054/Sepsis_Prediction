{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "153aa10e",
   "metadata": {},
   "source": [
    "# üîß Step 2: Data Preprocessing and Feature Engineering\n",
    "## Advanced Data Transformation for Sepsis Prediction\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **Objectives**\n",
    "- **Data Cleaning**: Handle missing values with clinical-informed strategies\n",
    "- **Feature Engineering**: Create temporal and clinical domain features\n",
    "- **Data Normalization**: Standardize features for machine learning\n",
    "- **Quality Assurance**: Validate preprocessing pipeline integrity\n",
    "\n",
    "---\n",
    "\n",
    "### üè• **Clinical-Informed Preprocessing Strategy**\n",
    "\n",
    "#### **Missing Value Handling** ü©∫\n",
    "| **Clinical Context** | **Imputation Strategy** | **Rationale** |\n",
    "|---------------------|------------------------|---------------|\n",
    "| **Vital Signs** | Forward-fill + Clinical bounds | Maintains physiological continuity |\n",
    "| **Lab Values** | Median + Time-decay | Reflects testing frequency patterns |\n",
    "| **Blood Gas** | Interpolation | Captures respiratory dynamics |\n",
    "| **Demographics** | Mode/Median | Stable patient characteristics |\n",
    "\n",
    "#### **Feature Engineering Categories** üß¨\n",
    "1. **Temporal Features**: Trends, slopes, variability measures\n",
    "2. **Clinical Ratios**: Shock index, oxygen ratios, perfusion indicators\n",
    "3. **Statistical Features**: Rolling statistics, percentiles, outlier indicators\n",
    "4. **Time-Since Features**: Time since abnormal values, admission time\n",
    "5. **Interaction Features**: Multi-organ system interactions\n",
    "\n",
    "---\n",
    "\n",
    "### üî¨ **Advanced Feature Engineering**\n",
    "\n",
    "#### **Temporal Dynamics** ‚è±Ô∏è\n",
    "- **Trend Analysis**: Slope calculations over sliding windows\n",
    "- **Variability Metrics**: Standard deviation, coefficient of variation\n",
    "- **Change Point Detection**: Sudden physiological changes\n",
    "- **Time-Series Decomposition**: Trend, seasonality, residuals\n",
    "\n",
    "#### **Clinical Scoring Systems** üìä\n",
    "- **SOFA-inspired Features**: Organ dysfunction indicators\n",
    "- **NEWS-based Features**: Early warning score components\n",
    "- **Custom Sepsis Indicators**: Domain-specific risk markers\n",
    "- **Multi-organ Integration**: Cross-system interaction patterns\n",
    "\n",
    "#### **Statistical Transformations** üìà\n",
    "- **Normalization**: Z-score, Min-Max, Robust scaling\n",
    "- **Distribution Adjustment**: Log, Box-Cox transformations\n",
    "- **Outlier Handling**: Clinical bounds, statistical methods\n",
    "- **Feature Scaling**: Unit standardization, clinical range mapping\n",
    "\n",
    "---\n",
    "\n",
    "### üìã **Pipeline Architecture**\n",
    "1. **Data Loading & Validation**\n",
    "2. **Missing Value Analysis & Imputation**\n",
    "3. **Temporal Feature Engineering**\n",
    "4. **Clinical Feature Creation**\n",
    "5. **Statistical Transformations**\n",
    "6. **Feature Selection & Validation**\n",
    "7. **Data Export for Modeling**\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **Expected Outputs**\n",
    "- Clean, imputed dataset ready for modeling\n",
    "- Rich feature set with temporal and clinical insights\n",
    "- Preprocessing pipeline for production deployment\n",
    "- Feature importance and correlation analysis\n",
    "- Data quality validation reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a394a243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Butterworth filtering modules imported successfully!\n",
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy import stats\n",
    "from scipy.interpolate import interp1d\n",
    "import joblib\n",
    "\n",
    "# Import Butterworth filtering modules\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\sachi\\Desktop\\Sepsis STFT\\signal_processing\")\n",
    "try:\n",
    "    from butterworth_filters import ButterworthProcessor\n",
    "    from enhanced_stft_integration import EnhancedSTFTProcessor\n",
    "    BUTTERWORTH_AVAILABLE = True\n",
    "    print(\"‚úÖ Butterworth filtering modules imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Butterworth modules not available: {e}\")\n",
    "    BUTTERWORTH_AVAILABLE = False\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be1172a",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Environment Setup and Data Loading\n",
    "\n",
    "Initializing the preprocessing environment with advanced libraries for clinical data transformation and feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff6ce646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: C:\\Users\\sachi\\Desktop\\Sepsis STFT\\data\\raw\\training_setA (1)\n",
      "Processed data will be saved to: C:\\Users\\sachi\\Desktop\\Sepsis STFT\\data\\processed\n",
      "Models will be saved to: C:\\Users\\sachi\\Desktop\\Sepsis STFT\\models\n"
     ]
    }
   ],
   "source": [
    "# Configuration and paths\n",
    "DATA_PATH = r\"C:\\Users\\sachi\\Desktop\\Sepsis STFT\\data\\raw\\training_setA (1)\"\n",
    "PROCESSED_DATA_PATH = r\"C:\\Users\\sachi\\Desktop\\Sepsis STFT\\data\\processed\"\n",
    "MODEL_PATH = r\"C:\\Users\\sachi\\Desktop\\Sepsis STFT\\models\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Processed data will be saved to: {PROCESSED_DATA_PATH}\")\n",
    "print(f\"Models will be saved to: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c044f9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data subset for preprocessing development...\n",
      "Loading 1000 patient files...\n",
      "Loaded 0/1000 files...\n",
      "Successfully loaded 1000 files\n",
      "Combined dataset shape: (38809, 42)\n",
      "Data loaded successfully: (38809, 42)\n",
      "Unique patients: 1000\n",
      "Successfully loaded 1000 files\n",
      "Combined dataset shape: (38809, 42)\n",
      "Data loaded successfully: (38809, 42)\n",
      "Unique patients: 1000\n"
     ]
    }
   ],
   "source": [
    "# Data loading functions\n",
    "def load_psv_file(filepath):\n",
    "    \"\"\"Load a single PSV file and add patient ID\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, sep='|')\n",
    "        patient_id = os.path.basename(filepath).replace('.psv', '')\n",
    "        df['PatientID'] = patient_id\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_all_data(data_path, max_patients=None):\n",
    "    \"\"\"Load all PSV files and combine them\"\"\"\n",
    "    psv_files = glob.glob(os.path.join(data_path, \"*.psv\"))\n",
    "    \n",
    "    if max_patients:\n",
    "        psv_files = psv_files[:max_patients]\n",
    "    \n",
    "    print(f\"Loading {len(psv_files)} patient files...\")\n",
    "    \n",
    "    data_list = []\n",
    "    failed_files = []\n",
    "    \n",
    "    for i, file in enumerate(psv_files):\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Loaded {i}/{len(psv_files)} files...\")\n",
    "        \n",
    "        df = load_psv_file(file)\n",
    "        if df is not None:\n",
    "            data_list.append(df)\n",
    "        else:\n",
    "            failed_files.append(file)\n",
    "    \n",
    "    if failed_files:\n",
    "        print(f\"Failed to load {len(failed_files)} files\")\n",
    "    \n",
    "    if data_list:\n",
    "        combined_data = pd.concat(data_list, ignore_index=True)\n",
    "        print(f\"Successfully loaded {len(data_list)} files\")\n",
    "        print(f\"Combined dataset shape: {combined_data.shape}\")\n",
    "        return combined_data\n",
    "    else:\n",
    "        raise ValueError(\"No data loaded successfully\")\n",
    "\n",
    "# Load subset for development (first 1000 patients)\n",
    "print(\"Loading data subset for preprocessing development...\")\n",
    "data = load_all_data(DATA_PATH, max_patients=1000)\n",
    "print(f\"Data loaded successfully: {data.shape}\")\n",
    "print(f\"Unique patients: {data['PatientID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff701154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature groups defined:\n",
      "- Vital signs: 7\n",
      "- Lab values: 20\n",
      "- Gas analysis: 7\n",
      "- Demographics: 2\n",
      "- Clinical context: 4\n"
     ]
    }
   ],
   "source": [
    "# Define feature groups for medical domain knowledge\n",
    "VITAL_SIGNS = ['HR', 'O2Sat', 'Temp', 'SBP', 'MAP', 'DBP', 'Resp']\n",
    "LAB_VALUES = ['AST', 'BUN', 'Alkalinephos', 'Calcium', 'Chloride', 'Creatinine',\n",
    "              'Bilirubin_direct', 'Glucose', 'Lactate', 'Magnesium', 'Phosphate',\n",
    "              'Potassium', 'Bilirubin_total', 'TroponinI', 'Hct', 'Hgb', 'PTT',\n",
    "              'WBC', 'Fibrinogen', 'Platelets']\n",
    "GAS_ANALYSIS = ['EtCO2', 'BaseExcess', 'HCO3', 'FiO2', 'pH', 'PaCO2', 'SaO2']\n",
    "DEMOGRAPHICS = ['Age', 'Gender']\n",
    "CLINICAL_CONTEXT = ['Unit1', 'Unit2', 'HospAdmTime', 'ICULOS']\n",
    "TARGET = ['SepsisLabel']\n",
    "\n",
    "# Medical reference ranges for outlier detection\n",
    "MEDICAL_RANGES = {\n",
    "    'HR': (30, 200),\n",
    "    'O2Sat': (70, 100),\n",
    "    'Temp': (30, 45),\n",
    "    'SBP': (50, 300),\n",
    "    'MAP': (30, 200),\n",
    "    'DBP': (20, 150),\n",
    "    'Resp': (5, 50),\n",
    "    'Age': (0, 120),\n",
    "    'pH': (6.5, 8.0),\n",
    "    'Glucose': (20, 800)\n",
    "}\n",
    "\n",
    "# Normal values for medical imputation\n",
    "NORMAL_VALUES = {\n",
    "    'HR': 80,\n",
    "    'O2Sat': 98,\n",
    "    'Temp': 36.5,\n",
    "    'SBP': 120,\n",
    "    'MAP': 90,\n",
    "    'DBP': 80,\n",
    "    'Resp': 16,\n",
    "    'pH': 7.4,\n",
    "    'FiO2': 0.21\n",
    "}\n",
    "\n",
    "print(f\"Feature groups defined:\")\n",
    "print(f\"- Vital signs: {len(VITAL_SIGNS)}\")\n",
    "print(f\"- Lab values: {len(LAB_VALUES)}\")\n",
    "print(f\"- Gas analysis: {len(GAS_ANALYSIS)}\")\n",
    "print(f\"- Demographics: {len(DEMOGRAPHICS)}\")\n",
    "print(f\"- Clinical context: {len(CLINICAL_CONTEXT)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf2998e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA QUALITY ASSESSMENT ===\n",
      "\n",
      "Missing data summary:\n",
      "Features with >50% missing: 28\n",
      "EtCO2               100.00\n",
      "TroponinI            99.84\n",
      "Bilirubin_direct     99.84\n",
      "Fibrinogen           99.15\n",
      "Bilirubin_total      98.86\n",
      "Alkalinephos         98.54\n",
      "AST                  98.50\n",
      "Lactate              96.40\n",
      "Calcium              95.32\n",
      "PTT                  95.31\n",
      "dtype: float64\n",
      "\n",
      "Duplicate rows: 0\n",
      "\n",
      "Data types:\n",
      "float64    38\n",
      "int64       3\n",
      "object      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Data quality assessment and cleaning\n",
    "def assess_data_quality(df):\n",
    "    \"\"\"Assess and report data quality issues\"\"\"\n",
    "    print(\"=== DATA QUALITY ASSESSMENT ===\")\n",
    "    \n",
    "    # Missing data analysis\n",
    "    missing_data = df.isnull().sum().sort_values(ascending=False)\n",
    "    missing_percent = (missing_data / len(df) * 100).round(2)\n",
    "    \n",
    "    print(f\"\\nMissing data summary:\")\n",
    "    high_missing = missing_percent[missing_percent > 50]\n",
    "    if len(high_missing) > 0:\n",
    "        print(f\"Features with >50% missing: {len(high_missing)}\")\n",
    "        print(high_missing.head(10))\n",
    "    \n",
    "    # Duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\nDuplicate rows: {duplicates}\")\n",
    "    \n",
    "    # Data types\n",
    "    print(f\"\\nData types:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    return missing_data, missing_percent\n",
    "\n",
    "missing_data, missing_percent = assess_data_quality(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e762d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OUTLIER DETECTION ===\n",
      "O2Sat: 22 outliers outside [70, 100]\n",
      "Temp: 2 outliers outside [30, 45]\n",
      "SBP: 5 outliers outside [50, 300]\n",
      "MAP: 17 outliers outside [30, 200]\n",
      "DBP: 5 outliers outside [20, 150]\n",
      "Resp: 34 outliers outside [5, 50]\n",
      "Glucose: 3 outliers outside [20, 800]\n",
      "\n",
      "Data shape after outlier treatment: (38809, 42)\n"
     ]
    }
   ],
   "source": [
    "# Outlier detection and treatment using medical knowledge\n",
    "def detect_medical_outliers(df, feature_ranges):\n",
    "    \"\"\"Detect outliers using medical reference ranges\"\"\"\n",
    "    outlier_counts = {}\n",
    "    \n",
    "    for feature, (min_val, max_val) in feature_ranges.items():\n",
    "        if feature in df.columns:\n",
    "            outliers = ((df[feature] < min_val) | (df[feature] > max_val))\n",
    "            outlier_count = outliers.sum()\n",
    "            outlier_counts[feature] = outlier_count\n",
    "            \n",
    "            if outlier_count > 0:\n",
    "                print(f\"{feature}: {outlier_count} outliers outside [{min_val}, {max_val}]\")\n",
    "    \n",
    "    return outlier_counts\n",
    "\n",
    "def treat_outliers(df, feature_ranges, method='clip'):\n",
    "    \"\"\"Treat outliers using medical reference ranges\"\"\"\n",
    "    df_treated = df.copy()\n",
    "    \n",
    "    for feature, (min_val, max_val) in feature_ranges.items():\n",
    "        if feature in df_treated.columns:\n",
    "            if method == 'clip':\n",
    "                df_treated[feature] = df_treated[feature].clip(min_val, max_val)\n",
    "            elif method == 'remove':\n",
    "                mask = (df_treated[feature] >= min_val) & (df_treated[feature] <= max_val)\n",
    "                df_treated = df_treated[mask]\n",
    "    \n",
    "    return df_treated\n",
    "\n",
    "# Detect outliers\n",
    "print(\"=== OUTLIER DETECTION ===\")\n",
    "outlier_counts = detect_medical_outliers(data, MEDICAL_RANGES)\n",
    "\n",
    "# Treat outliers by clipping to medical ranges\n",
    "data_cleaned = treat_outliers(data, MEDICAL_RANGES, method='clip')\n",
    "print(f\"\\nData shape after outlier treatment: {data_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "702211ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MISSING VALUE IMPUTATION ===\n",
      "Before imputation - Missing values: 1086709\n",
      "After imputation - Missing values: 38809\n",
      "Final missing values: 38809\n",
      "After imputation - Missing values: 38809\n",
      "Final missing values: 38809\n"
     ]
    }
   ],
   "source": [
    "# Missing value imputation with medical domain knowledge\n",
    "class MedicalImputer:\n",
    "    \"\"\"Custom imputer for medical time series data\"\"\"\n",
    "    \n",
    "    def __init__(self, strategy='medical_forward_fill'):\n",
    "        self.strategy = strategy\n",
    "        self.imputation_values = {}\n",
    "    \n",
    "    def fit(self, df):\n",
    "        \"\"\"Fit the imputer to the data\"\"\"\n",
    "        # Calculate median values for each feature\n",
    "        for col in df.columns:\n",
    "            if col not in ['PatientID', 'SepsisLabel', 'ICULOS', 'HospAdmTime']:\n",
    "                if col in NORMAL_VALUES:\n",
    "                    self.imputation_values[col] = NORMAL_VALUES[col]\n",
    "                else:\n",
    "                    self.imputation_values[col] = df[col].median()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \"\"\"Apply imputation to the data\"\"\"\n",
    "        df_imputed = df.copy()\n",
    "        \n",
    "        # Sort by patient and time for forward fill\n",
    "        df_imputed = df_imputed.sort_values(['PatientID', 'ICULOS'])\n",
    "        \n",
    "        if self.strategy == 'medical_forward_fill':\n",
    "            # Forward fill within each patient\n",
    "            for patient_id in df_imputed['PatientID'].unique():\n",
    "                patient_mask = df_imputed['PatientID'] == patient_id\n",
    "                \n",
    "                # Forward fill for each feature group\n",
    "                for feature_group in [VITAL_SIGNS, LAB_VALUES, GAS_ANALYSIS]:\n",
    "                    available_features = [f for f in feature_group if f in df_imputed.columns]\n",
    "                    df_imputed.loc[patient_mask, available_features] = df_imputed.loc[patient_mask, available_features].fillna(method='ffill')\n",
    "                \n",
    "                # Fill remaining missing values with medical normal values or median\n",
    "                for col in df_imputed.columns:\n",
    "                    if col in self.imputation_values:\n",
    "                        df_imputed.loc[patient_mask, col] = df_imputed.loc[patient_mask, col].fillna(self.imputation_values[col])\n",
    "        \n",
    "        return df_imputed\n",
    "    \n",
    "    def fit_transform(self, df):\n",
    "        return self.fit(df).transform(df)\n",
    "\n",
    "# Apply medical imputation\n",
    "print(\"=== MISSING VALUE IMPUTATION ===\")\n",
    "print(f\"Before imputation - Missing values: {data_cleaned.isnull().sum().sum()}\")\n",
    "\n",
    "imputer = MedicalImputer(strategy='medical_forward_fill')\n",
    "data_imputed = imputer.fit_transform(data_cleaned)\n",
    "\n",
    "print(f\"After imputation - Missing values: {data_imputed.isnull().sum().sum()}\")\n",
    "\n",
    "# Fill any remaining missing values with median\n",
    "for col in data_imputed.columns:\n",
    "    if data_imputed[col].isnull().any() and col not in ['PatientID']:\n",
    "        if data_imputed[col].dtype in ['float64', 'int64']:\n",
    "            data_imputed[col].fillna(data_imputed[col].median(), inplace=True)\n",
    "        else:\n",
    "            data_imputed[col].fillna(data_imputed[col].mode()[0], inplace=True)\n",
    "\n",
    "print(f\"Final missing values: {data_imputed.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f1daa75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporal features...\n",
      "Creating statistical features...\n",
      "Created temporal features. New shape: (38809, 122)\n",
      "\n",
      "Feature engineering completed. Shape: (38809, 122)\n",
      "New features created: 80\n",
      "Creating statistical features...\n",
      "Created temporal features. New shape: (38809, 122)\n",
      "\n",
      "Feature engineering completed. Shape: (38809, 122)\n",
      "New features created: 80\n"
     ]
    }
   ],
   "source": [
    "# Feature engineering for temporal patterns\n",
    "def create_temporal_features(df):\n",
    "    \"\"\"Create temporal features for time series analysis\"\"\"\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Sort by patient and time\n",
    "    df_features = df_features.sort_values(['PatientID', 'ICULOS'])\n",
    "    \n",
    "    print(\"Creating temporal features...\")\n",
    "    \n",
    "    # Time-based features\n",
    "    df_features['Hour_in_ICU'] = df_features['ICULOS']\n",
    "    df_features['Time_since_admission'] = df_features['ICULOS'] + df_features['HospAdmTime']\n",
    "    \n",
    "    # Cyclical time features\n",
    "    df_features['Hour_sin'] = np.sin(2 * np.pi * df_features['ICULOS'] / 24)\n",
    "    df_features['Hour_cos'] = np.cos(2 * np.pi * df_features['ICULOS'] / 24)\n",
    "    \n",
    "    # Patient-level aggregated features\n",
    "    for patient_id in df_features['PatientID'].unique():\n",
    "        patient_mask = df_features['PatientID'] == patient_id\n",
    "        patient_data = df_features[patient_mask].copy()\n",
    "        \n",
    "        # Rolling window features (3-hour and 6-hour windows)\n",
    "        for window in [3, 6]:\n",
    "            for feature in VITAL_SIGNS + ['Lactate', 'WBC', 'Glucose']:\n",
    "                if feature in df_features.columns:\n",
    "                    # Rolling mean\n",
    "                    rolling_mean = patient_data[feature].rolling(window=window, min_periods=1).mean()\n",
    "                    df_features.loc[patient_mask, f'{feature}_rolling_mean_{window}h'] = rolling_mean\n",
    "                    \n",
    "                    # Rolling std\n",
    "                    rolling_std = patient_data[feature].rolling(window=window, min_periods=1).std()\n",
    "                    df_features.loc[patient_mask, f'{feature}_rolling_std_{window}h'] = rolling_std.fillna(0)\n",
    "                    \n",
    "                    # Rolling max/min\n",
    "                    rolling_max = patient_data[feature].rolling(window=window, min_periods=1).max()\n",
    "                    rolling_min = patient_data[feature].rolling(window=window, min_periods=1).min()\n",
    "                    df_features.loc[patient_mask, f'{feature}_rolling_range_{window}h'] = rolling_max - rolling_min\n",
    "        \n",
    "        # Trend features (slope over last 3 hours)\n",
    "        for feature in VITAL_SIGNS + ['Lactate', 'WBC']:\n",
    "            if feature in df_features.columns:\n",
    "                # Calculate slope using linear regression over rolling window\n",
    "                trends = []\n",
    "                for i in range(len(patient_data)):\n",
    "                    start_idx = max(0, i-2)  # 3-hour window\n",
    "                    y_vals = patient_data[feature].iloc[start_idx:i+1].values\n",
    "                    x_vals = np.arange(len(y_vals))\n",
    "                    \n",
    "                    if len(y_vals) > 1:\n",
    "                        slope, _, _, _, _ = stats.linregress(x_vals, y_vals)\n",
    "                        trends.append(slope)\n",
    "                    else:\n",
    "                        trends.append(0)\n",
    "                \n",
    "                df_features.loc[patient_mask, f'{feature}_trend_3h'] = trends\n",
    "    \n",
    "    # Statistical features\n",
    "    print(\"Creating statistical features...\")\n",
    "    \n",
    "    # SOFA-like composite scores\n",
    "    # Cardiovascular SOFA component\n",
    "    df_features['Cardiovascular_score'] = 0\n",
    "    df_features.loc[df_features['MAP'] < 70, 'Cardiovascular_score'] = 1\n",
    "    df_features.loc[df_features['MAP'] < 60, 'Cardiovascular_score'] = 2\n",
    "    \n",
    "    # Respiratory SOFA component\n",
    "    df_features['Respiratory_score'] = 0\n",
    "    pf_ratio = df_features['O2Sat'] / (df_features['FiO2'] + 0.01)  # Approximate P/F ratio\n",
    "    df_features.loc[pf_ratio < 400, 'Respiratory_score'] = 1\n",
    "    df_features.loc[pf_ratio < 300, 'Respiratory_score'] = 2\n",
    "    df_features.loc[pf_ratio < 200, 'Respiratory_score'] = 3\n",
    "    \n",
    "    # Renal SOFA component\n",
    "    df_features['Renal_score'] = 0\n",
    "    df_features.loc[df_features['Creatinine'] > 1.2, 'Renal_score'] = 1\n",
    "    df_features.loc[df_features['Creatinine'] > 2.0, 'Renal_score'] = 2\n",
    "    df_features.loc[df_features['Creatinine'] > 3.5, 'Renal_score'] = 3\n",
    "    \n",
    "    # Combine scores\n",
    "    df_features['Total_SOFA_approx'] = (df_features['Cardiovascular_score'] + \n",
    "                                        df_features['Respiratory_score'] + \n",
    "                                        df_features['Renal_score'])\n",
    "    \n",
    "    # Additional medical ratios and indices\n",
    "    df_features['Shock_index'] = df_features['HR'] / (df_features['SBP'] + 0.1)\n",
    "    df_features['Modified_shock_index'] = df_features['HR'] / (df_features['MAP'] + 0.1)\n",
    "    df_features['Oxygen_index'] = df_features['O2Sat'] / (df_features['FiO2'] + 0.01)\n",
    "    \n",
    "    print(f\"Created temporal features. New shape: {df_features.shape}\")\n",
    "    return df_features\n",
    "\n",
    "# Create temporal features\n",
    "data_with_features = create_temporal_features(data_imputed)\n",
    "print(f\"\\nFeature engineering completed. Shape: {data_with_features.shape}\")\n",
    "print(f\"New features created: {data_with_features.shape[1] - data_imputed.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80021316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Signal Processing with Butterworth Filtering\n",
    "def apply_butterworth_filtering(df):\n",
    "    \"\"\"Apply clinical-grade Butterworth filtering to physiological signals\"\"\"\n",
    "    if not BUTTERWORTH_AVAILABLE:\n",
    "        print(\"‚ö†Ô∏è Butterworth filtering not available - using original signals\")\n",
    "        return df\n",
    "    \n",
    "    print(\"=== BUTTERWORTH SIGNAL FILTERING ===\")\n",
    "    print(\"üîß Applying clinical-grade signal filtering for sepsis prediction...\")\n",
    "    \n",
    "    # Initialize Butterworth processor\n",
    "    butterworth = ButterworthProcessor(sampling_rate=100)  # Assuming 100 Hz equivalent\n",
    "    \n",
    "    # Define signal mapping for clinical optimization\n",
    "    signal_mapping = {\n",
    "        'HR': 'heart_rate',\n",
    "        'SBP': 'blood_pressure',\n",
    "        'DBP': 'blood_pressure', \n",
    "        'MAP': 'blood_pressure',\n",
    "        'Temp': 'temperature',\n",
    "        'Resp': 'respiratory',\n",
    "        'O2Sat': 'heart_rate',  # Treat similar to HR for filtering\n",
    "    }\n",
    "    \n",
    "    df_filtered = df.copy()\n",
    "    filtering_summary = {}\n",
    "    \n",
    "    # Process each patient separately to maintain temporal coherence\n",
    "    unique_patients = df['PatientID'].unique()\n",
    "    print(f\"Processing {len(unique_patients)} patients with Butterworth filtering...\")\n",
    "    \n",
    "    for i, patient_id in enumerate(unique_patients):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"  Processed {i}/{len(unique_patients)} patients...\")\n",
    "        \n",
    "        # Get patient data sorted by time\n",
    "        patient_mask = df['PatientID'] == patient_id\n",
    "        patient_data = df[patient_mask].sort_values('ICULOS')\n",
    "        \n",
    "        if len(patient_data) < 3:  # Need minimum samples for filtering\n",
    "            continue\n",
    "        \n",
    "        # Apply Butterworth filtering to each physiological signal\n",
    "        for signal_name, signal_type in signal_mapping.items():\n",
    "            if signal_name in patient_data.columns:\n",
    "                signal_values = patient_data[signal_name].values\n",
    "                \n",
    "                # Skip if signal has insufficient variation\n",
    "                if np.std(signal_values) < 1e-6:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Apply Butterworth filtering\n",
    "                    filter_result = butterworth.process_physiological_signal(\n",
    "                        signal_values, \n",
    "                        signal_type\n",
    "                    )\n",
    "                    \n",
    "                    # Store filtered signal\n",
    "                    filtered_signal = filter_result['filtered_signal']\n",
    "                    df_filtered.loc[patient_mask, f'{signal_name}_filtered'] = filtered_signal\n",
    "                    \n",
    "                    # Track filtering performance\n",
    "                    if signal_name not in filtering_summary:\n",
    "                        filtering_summary[signal_name] = {\n",
    "                            'noise_reductions': [],\n",
    "                            'patients_processed': 0,\n",
    "                            'signal_type': signal_type\n",
    "                        }\n",
    "                    \n",
    "                    filtering_summary[signal_name]['noise_reductions'].append(\n",
    "                        filter_result['noise_reduction']\n",
    "                    )\n",
    "                    filtering_summary[signal_name]['patients_processed'] += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Filtering failed for {patient_id}-{signal_name}: {e}\")\n",
    "                    # Use original signal if filtering fails\n",
    "                    df_filtered.loc[patient_mask, f'{signal_name}_filtered'] = signal_values\n",
    "    \n",
    "    # Print filtering summary\n",
    "    print(f\"\\nüîß Butterworth Filtering Summary:\")\n",
    "    for signal_name, summary in filtering_summary.items():\n",
    "        if summary['noise_reductions']:\n",
    "            avg_noise_reduction = np.mean(summary['noise_reductions']) * 100\n",
    "            print(f\"   {signal_name} ({summary['signal_type']}): \"\n",
    "                  f\"{avg_noise_reduction:.1f}% avg noise reduction \"\n",
    "                  f\"({summary['patients_processed']} patients)\")\n",
    "    \n",
    "    # Create enhanced features from filtered signals\n",
    "    print(f\"\\nüìä Creating enhanced features from filtered signals...\")\n",
    "    \n",
    "    # Replace original signals with filtered versions for downstream processing\n",
    "    for signal_name in signal_mapping.keys():\n",
    "        if f'{signal_name}_filtered' in df_filtered.columns:\n",
    "            # Keep both original and filtered versions\n",
    "            df_filtered[f'{signal_name}_original'] = df_filtered[signal_name]\n",
    "            df_filtered[signal_name] = df_filtered[f'{signal_name}_filtered']\n",
    "    \n",
    "    print(f\"‚úÖ Butterworth filtering completed!\")\n",
    "    print(f\"   Original features preserved with '_original' suffix\")\n",
    "    print(f\"   Main signals now use Butterworth-filtered versions\")\n",
    "    print(f\"   Dataset shape: {df_filtered.shape}\")\n",
    "    \n",
    "    return df_filtered, filtering_summary\n",
    "\n",
    "# Apply Butterworth filtering to the dataset\n",
    "if BUTTERWORTH_AVAILABLE:\n",
    "    data_butterworth_filtered, filter_summary = apply_butterworth_filtering(data_with_features)\n",
    "    \n",
    "    # Use the filtered data for further processing\n",
    "    data_with_features = data_butterworth_filtered\n",
    "    \n",
    "    print(f\"\\nüéØ Enhanced preprocessing pipeline with Butterworth filtering active!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Continuing with standard preprocessing (Butterworth not available)\")\n",
    "    filter_summary = {}\n",
    "\n",
    "print(f\"Data ready for scaling: {data_with_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a9a8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Butterworth Filtering Demo and Visualization\n",
    "def demonstrate_butterworth_filtering():\n",
    "    \"\"\"Demonstrate Butterworth filtering with real patient data\"\"\"\n",
    "    if not BUTTERWORTH_AVAILABLE:\n",
    "        print(\"‚ö†Ô∏è Butterworth filtering not available - skipping demonstration\")\n",
    "        return\n",
    "    \n",
    "    print(\"üîß BUTTERWORTH FILTERING DEMONSTRATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize Butterworth processor\n",
    "    butterworth = ButterworthProcessor(sampling_rate=100)\n",
    "    \n",
    "    # Get a sample patient with sufficient data\n",
    "    sample_patient = None\n",
    "    for patient_id in data_with_features['PatientID'].unique()[:10]:\n",
    "        patient_data = data_with_features[data_with_features['PatientID'] == patient_id]\n",
    "        if len(patient_data) >= 50 and 'HR' in patient_data.columns:  # At least 50 time points\n",
    "            sample_patient = patient_data\n",
    "            break\n",
    "    \n",
    "    if sample_patient is None:\n",
    "        print(\"‚ùå No suitable patient data found for demonstration\")\n",
    "        return\n",
    "    \n",
    "    # Demonstrate on Heart Rate signal\n",
    "    hr_signal = sample_patient['HR'].values\n",
    "    if len(hr_signal) < 10:\n",
    "        print(\"‚ùå Insufficient heart rate data\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìä Sample patient: {sample_patient['PatientID'].iloc[0]}\")\n",
    "    print(f\"   Data points: {len(hr_signal)}\")\n",
    "    print(f\"   HR range: {hr_signal.min():.1f} - {hr_signal.max():.1f} BPM\")\n",
    "    \n",
    "    # Apply Butterworth filtering\n",
    "    try:\n",
    "        filter_result = butterworth.process_physiological_signal(hr_signal, 'heart_rate')\n",
    "        \n",
    "        print(f\"\\nüîß Butterworth Filtering Results:\")\n",
    "        print(f\"   Noise reduction: {filter_result['noise_reduction']*100:.1f}%\")\n",
    "        print(f\"   Signal quality improvement: {filter_result['quality_improvement']:.3f}\")\n",
    "        print(f\"   Filter type: {filter_result['signal_type']}\")\n",
    "        \n",
    "        # Quick visualization\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "        \n",
    "        time_points = np.arange(len(hr_signal))\n",
    "        \n",
    "        # Original vs Filtered\n",
    "        ax1.plot(time_points, hr_signal, 'b-', alpha=0.7, label='Original HR', linewidth=1)\n",
    "        ax1.plot(time_points, filter_result['filtered_signal'], 'r-', label='Butterworth Filtered', linewidth=2)\n",
    "        ax1.set_title('üîß Heart Rate Signal: Original vs Butterworth Filtered')\n",
    "        ax1.set_xlabel('Time Points')\n",
    "        ax1.set_ylabel('Heart Rate (BPM)')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Difference (noise removed)\n",
    "        noise_removed = hr_signal - filter_result['filtered_signal']\n",
    "        ax2.plot(time_points, noise_removed, 'g-', alpha=0.8, label='Noise Removed')\n",
    "        ax2.set_title('üìä Noise Removed by Butterworth Filter')\n",
    "        ax2.set_xlabel('Time Points')\n",
    "        ax2.set_ylabel('Amplitude Difference')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"‚úÖ Butterworth filtering demonstration completed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Demonstration failed: {e}\")\n",
    "\n",
    "# Run the demonstration if we have data\n",
    "if 'data_with_features' in locals() and BUTTERWORTH_AVAILABLE:\n",
    "    demonstrate_butterworth_filtering()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping Butterworth demonstration - data or modules not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b55cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb4c206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling preparation:\n",
      "- Robust scaling: 96 features\n",
      "- Standard scaling: 8 features\n",
      "- MinMax scaling: 7 features\n",
      "\n",
      "Data scaling completed. Shape: (38809, 122)\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Data scaling with Butterworth-filtered signals\n",
    "def prepare_scaling_with_butterworth(df, feature_groups):\n",
    "    \"\"\"Prepare data for scaling by feature groups (enhanced for Butterworth filtering)\"\"\"\n",
    "    scalers = {}\n",
    "    \n",
    "    # Separate features that need different scaling\n",
    "    robust_features = []  # For features with outliers (including filtered signals)\n",
    "    standard_features = []  # For normally distributed features\n",
    "    minmax_features = []  # For bounded features\n",
    "    \n",
    "    # Handle original feature groups\n",
    "    for feature_group_name, features in feature_groups.items():\n",
    "        available_features = [f for f in features if f in df.columns]\n",
    "        \n",
    "        if feature_group_name in ['VITAL_SIGNS', 'LAB_VALUES']:\n",
    "            robust_features.extend(available_features)\n",
    "        elif feature_group_name in ['DEMOGRAPHICS']:\n",
    "            standard_features.extend(available_features)\n",
    "        else:\n",
    "            minmax_features.extend(available_features)\n",
    "    \n",
    "    # Add Butterworth-filtered features to robust scaling\n",
    "    # These are the cleaned physiological signals that benefit from robust scaling\n",
    "    butterworth_features = [col for col in df.columns if col.endswith('_filtered')]\n",
    "    robust_features.extend(butterworth_features)\n",
    "    \n",
    "    # Add rolling features to robust scaling\n",
    "    rolling_features = [col for col in df.columns if 'rolling' in col or 'trend' in col]\n",
    "    robust_features.extend(rolling_features)\n",
    "    \n",
    "    # Add composite scores to standard scaling\n",
    "    score_features = [col for col in df.columns if 'score' in col or 'index' in col]\n",
    "    standard_features.extend(score_features)\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    robust_features = list(dict.fromkeys(robust_features))\n",
    "    standard_features = list(dict.fromkeys(standard_features))\n",
    "    minmax_features = list(dict.fromkeys(minmax_features))\n",
    "    \n",
    "    return robust_features, standard_features, minmax_features\n",
    "\n",
    "# Prepare feature groups for scaling (enhanced for Butterworth)\n",
    "feature_groups = {\n",
    "    'VITAL_SIGNS': VITAL_SIGNS,\n",
    "    'LAB_VALUES': LAB_VALUES,\n",
    "    'GAS_ANALYSIS': GAS_ANALYSIS,\n",
    "    'DEMOGRAPHICS': DEMOGRAPHICS\n",
    "}\n",
    "\n",
    "robust_features, standard_features, minmax_features = prepare_scaling_with_butterworth(data_with_features, feature_groups)\n",
    "\n",
    "print(f\"Enhanced scaling preparation (with Butterworth filtering):\")\n",
    "print(f\"- Robust scaling: {len(robust_features)} features\")\n",
    "if BUTTERWORTH_AVAILABLE:\n",
    "    butterworth_count = len([f for f in robust_features if f.endswith('_filtered')])\n",
    "    print(f\"  ‚îî‚îÄ‚îÄ Including {butterworth_count} Butterworth-filtered signals\")\n",
    "print(f\"- Standard scaling: {len(standard_features)} features\")\n",
    "print(f\"- MinMax scaling: {len(minmax_features)} features\")\n",
    "\n",
    "# Create and fit scalers with enhanced feature handling\n",
    "scalers = {}\n",
    "data_scaled = data_with_features.copy()\n",
    "\n",
    "if robust_features:\n",
    "    scalers['robust'] = RobustScaler()\n",
    "    # Only scale features that exist in the dataset\n",
    "    existing_robust_features = [f for f in robust_features if f in data_scaled.columns]\n",
    "    if existing_robust_features:\n",
    "        data_scaled[existing_robust_features] = scalers['robust'].fit_transform(data_scaled[existing_robust_features])\n",
    "\n",
    "if standard_features:\n",
    "    scalers['standard'] = StandardScaler()\n",
    "    existing_standard_features = [f for f in standard_features if f in data_scaled.columns]\n",
    "    if existing_standard_features:\n",
    "        data_scaled[existing_standard_features] = scalers['standard'].fit_transform(data_scaled[existing_standard_features])\n",
    "\n",
    "if minmax_features:\n",
    "    scalers['minmax'] = MinMaxScaler()\n",
    "    existing_minmax_features = [f for f in minmax_features if f in data_scaled.columns]\n",
    "    if existing_minmax_features:\n",
    "        data_scaled[existing_minmax_features] = scalers['minmax'].fit_transform(data_scaled[existing_minmax_features])\n",
    "\n",
    "print(f\"\\n‚úÖ Enhanced data scaling completed with Butterworth integration!\")\n",
    "print(f\"   Dataset shape: {data_scaled.shape}\")\n",
    "if BUTTERWORTH_AVAILABLE and filter_summary:\n",
    "    print(f\"   Butterworth-enhanced signals: {len([f for f in data_scaled.columns if f.endswith('_filtered')])}\")\n",
    "    print(f\"   Signal quality improvements applied to preprocessing pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dde5adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CREATING ML DATASETS ===\n",
      "Creating ML dataset with sequence length: 12 hours\n",
      "Created 26863 sequences\n",
      "Sequence shape: (26863, 12, 117)\n",
      "Target distribution: [26207   656]\n",
      "\n",
      "Flattened dataset:\n",
      "X shape: (38809, 116)\n",
      "y distribution: [37945   864]\n"
     ]
    }
   ],
   "source": [
    "# Temporal data preparation for machine learning\n",
    "def create_ml_dataset(df, sequence_length=12, prediction_horizon=1):\n",
    "    \"\"\"Create ML dataset with sequences for temporal modeling\"\"\"\n",
    "    print(f\"Creating ML dataset with sequence length: {sequence_length} hours\")\n",
    "    \n",
    "    # Exclude non-feature columns\n",
    "    exclude_cols = ['PatientID', 'ICULOS', 'HospAdmTime', 'Unit1', 'Unit2']\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "    patient_ids = []\n",
    "    timestamps = []\n",
    "    \n",
    "    for patient_id in df['PatientID'].unique():\n",
    "        patient_data = df[df['PatientID'] == patient_id].sort_values('ICULOS')\n",
    "        \n",
    "        if len(patient_data) >= sequence_length + prediction_horizon:\n",
    "            for i in range(len(patient_data) - sequence_length - prediction_horizon + 1):\n",
    "                # Input sequence\n",
    "                sequence_data = patient_data.iloc[i:i+sequence_length]\n",
    "                X_sequence = sequence_data[feature_cols].values\n",
    "                \n",
    "                # Target (predict sepsis at prediction_horizon hours ahead)\n",
    "                target_idx = i + sequence_length + prediction_horizon - 1\n",
    "                y_target = patient_data.iloc[target_idx]['SepsisLabel']\n",
    "                \n",
    "                X_sequences.append(X_sequence)\n",
    "                y_sequences.append(y_target)\n",
    "                patient_ids.append(patient_id)\n",
    "                timestamps.append(patient_data.iloc[target_idx]['ICULOS'])\n",
    "    \n",
    "    X = np.array(X_sequences)\n",
    "    y = np.array(y_sequences)\n",
    "    \n",
    "    print(f\"Created {len(X)} sequences\")\n",
    "    print(f\"Sequence shape: {X.shape}\")\n",
    "    print(f\"Target distribution: {np.bincount(y)}\")\n",
    "    \n",
    "    return X, y, np.array(patient_ids), np.array(timestamps), feature_cols\n",
    "\n",
    "# Create both sequence and flattened datasets\n",
    "print(\"=== CREATING ML DATASETS ===\")\n",
    "\n",
    "# Sequence dataset for LSTM/RNN models\n",
    "X_seq, y_seq, patient_ids_seq, timestamps_seq, feature_names = create_ml_dataset(data_scaled, sequence_length=12)\n",
    "\n",
    "# Flattened dataset for traditional ML models\n",
    "exclude_cols = ['PatientID', 'ICULOS', 'HospAdmTime', 'Unit1', 'Unit2']\n",
    "feature_columns = [col for col in data_scaled.columns if col not in exclude_cols and col != 'SepsisLabel']\n",
    "\n",
    "X_flat = data_scaled[feature_columns].values\n",
    "y_flat = data_scaled['SepsisLabel'].values\n",
    "\n",
    "print(f\"\\nFlattened dataset:\")\n",
    "print(f\"X shape: {X_flat.shape}\")\n",
    "print(f\"y distribution: {np.bincount(y_flat)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d437203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEMPORAL TRAIN-VALIDATION-TEST SPLIT ===\n",
      "Split summary:\n",
      "- Train patients: 700 (70.0%)\n",
      "- Validation patients: 100 (10.0%)\n",
      "- Test patients: 200 (20.0%)\n",
      "\n",
      "Data distribution:\n",
      "- Train samples: 26714\n",
      "- Validation samples: 3923\n",
      "- Test samples: 8172\n",
      "\n",
      "Sepsis distribution:\n",
      "- Train: 2.27% sepsis cases\n",
      "- Validation: 2.22% sepsis cases\n",
      "- Test: 2.08% sepsis cases\n"
     ]
    }
   ],
   "source": [
    "# Temporal train-test split\n",
    "def temporal_train_test_split(df, test_size=0.2, validation_size=0.1):\n",
    "    \"\"\"Split data temporally by patients to avoid data leakage\"\"\"\n",
    "    # Get unique patients and their sepsis status\n",
    "    patient_info = df.groupby('PatientID').agg({\n",
    "        'SepsisLabel': 'max',\n",
    "        'ICULOS': 'max'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Stratified split by sepsis status\n",
    "    sepsis_patients = patient_info[patient_info['SepsisLabel'] == 1]['PatientID'].values\n",
    "    no_sepsis_patients = patient_info[patient_info['SepsisLabel'] == 0]['PatientID'].values\n",
    "    \n",
    "    # Split each group\n",
    "    np.random.shuffle(sepsis_patients)\n",
    "    np.random.shuffle(no_sepsis_patients)\n",
    "    \n",
    "    # Calculate split indices\n",
    "    n_sepsis_test = int(len(sepsis_patients) * test_size)\n",
    "    n_sepsis_val = int(len(sepsis_patients) * validation_size)\n",
    "    \n",
    "    n_no_sepsis_test = int(len(no_sepsis_patients) * test_size)\n",
    "    n_no_sepsis_val = int(len(no_sepsis_patients) * validation_size)\n",
    "    \n",
    "    # Create splits\n",
    "    test_patients = np.concatenate([\n",
    "        sepsis_patients[:n_sepsis_test],\n",
    "        no_sepsis_patients[:n_no_sepsis_test]\n",
    "    ])\n",
    "    \n",
    "    val_patients = np.concatenate([\n",
    "        sepsis_patients[n_sepsis_test:n_sepsis_test + n_sepsis_val],\n",
    "        no_sepsis_patients[n_no_sepsis_test:n_no_sepsis_test + n_no_sepsis_val]\n",
    "    ])\n",
    "    \n",
    "    train_patients = np.concatenate([\n",
    "        sepsis_patients[n_sepsis_test + n_sepsis_val:],\n",
    "        no_sepsis_patients[n_no_sepsis_test + n_no_sepsis_val:]\n",
    "    ])\n",
    "    \n",
    "    # Create boolean masks\n",
    "    train_mask = df['PatientID'].isin(train_patients)\n",
    "    val_mask = df['PatientID'].isin(val_patients)\n",
    "    test_mask = df['PatientID'].isin(test_patients)\n",
    "    \n",
    "    return train_mask, val_mask, test_mask, train_patients, val_patients, test_patients\n",
    "\n",
    "# Create temporal splits\n",
    "print(\"=== TEMPORAL TRAIN-VALIDATION-TEST SPLIT ===\")\n",
    "train_mask, val_mask, test_mask, train_patients, val_patients, test_patients = temporal_train_test_split(data_scaled)\n",
    "\n",
    "print(f\"Split summary:\")\n",
    "print(f\"- Train patients: {len(train_patients)} ({len(train_patients)/data_scaled['PatientID'].nunique()*100:.1f}%)\")\n",
    "print(f\"- Validation patients: {len(val_patients)} ({len(val_patients)/data_scaled['PatientID'].nunique()*100:.1f}%)\")\n",
    "print(f\"- Test patients: {len(test_patients)} ({len(test_patients)/data_scaled['PatientID'].nunique()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nData distribution:\")\n",
    "print(f\"- Train samples: {train_mask.sum()}\")\n",
    "print(f\"- Validation samples: {val_mask.sum()}\")\n",
    "print(f\"- Test samples: {test_mask.sum()}\")\n",
    "\n",
    "# Check sepsis distribution in each split\n",
    "print(f\"\\nSepsis distribution:\")\n",
    "for split_name, mask in [('Train', train_mask), ('Validation', val_mask), ('Test', test_mask)]:\n",
    "    split_data = data_scaled[mask]\n",
    "    sepsis_rate = split_data['SepsisLabel'].mean() * 100\n",
    "    print(f\"- {split_name}: {sepsis_rate:.2f}% sepsis cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b9213a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 02 COMPLETED ===\n",
      "‚úì Data loading and preprocessing pipeline established\n",
      "‚úì Missing value handling implemented\n",
      "‚úì Feature engineering completed\n",
      "‚úì Data scaling and normalization applied\n",
      "‚úì Train/validation/test splits created\n",
      "\n",
      "Step 02 completed successfully!\n",
      "Moving to Step 03 - Traditional ML Baseline Models\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Step 02 - Data Preprocessing with Butterworth Integration\n",
    "print(\"=== ENHANCED STEP 02 COMPLETED ===\")\n",
    "print(\"‚úì Data loading and preprocessing pipeline established\")\n",
    "print(\"‚úì Missing value handling implemented\")\n",
    "print(\"‚úì Feature engineering completed\")\n",
    "\n",
    "if BUTTERWORTH_AVAILABLE:\n",
    "    print(\"‚úÖ BUTTERWORTH SIGNAL FILTERING INTEGRATED!\")\n",
    "    print(\"   üîß Clinical-grade filtering applied to physiological signals\")\n",
    "    print(\"   üìä Signal quality improvements for sepsis prediction\")\n",
    "    print(\"   üéØ Enhanced feature quality for machine learning\")\n",
    "    \n",
    "    # Show filtering summary\n",
    "    if filter_summary:\n",
    "        print(f\"\\nüîß Butterworth Filtering Results:\")\n",
    "        for signal_name, summary in filter_summary.items():\n",
    "            if summary['noise_reductions']:\n",
    "                avg_reduction = np.mean(summary['noise_reductions']) * 100\n",
    "                print(f\"   ‚Ä¢ {signal_name}: {avg_reduction:.1f}% noise reduction\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Standard preprocessing (Butterworth filtering not available)\")\n",
    "\n",
    "print(\"‚úì Data scaling and normalization applied\")\n",
    "print(\"‚úì Train/validation/test splits created\")\n",
    "\n",
    "# Final dataset summary\n",
    "print(f\"\\nüìä Final Dataset Summary:\")\n",
    "print(f\"   Shape: {data_scaled.shape}\")\n",
    "print(f\"   Patients: {data_scaled['PatientID'].nunique()}\")\n",
    "print(f\"   Features: {data_scaled.shape[1] - 6}\")  # Exclude non-feature columns\n",
    "print(f\"   Sepsis cases: {data_scaled['SepsisLabel'].sum()} ({data_scaled['SepsisLabel'].mean()*100:.1f}%)\")\n",
    "\n",
    "if BUTTERWORTH_AVAILABLE:\n",
    "    butterworth_features = [col for col in data_scaled.columns if col.endswith('_filtered')]\n",
    "    if butterworth_features:\n",
    "        print(f\"   Butterworth-enhanced signals: {len(butterworth_features)}\")\n",
    "\n",
    "print(\"\\nüéØ ENHANCED PREPROCESSING PIPELINE READY!\")\n",
    "print(\"   Your sepsis prediction model now benefits from:\")\n",
    "print(\"   ‚Ä¢ Clinical-grade signal filtering\")\n",
    "print(\"   ‚Ä¢ Improved noise reduction\")\n",
    "print(\"   ‚Ä¢ Enhanced feature quality\")\n",
    "print(\"   ‚Ä¢ Better physiological signal representation\")\n",
    "\n",
    "print(\"\\nStep 02 completed successfully!\")\n",
    "print(\"Moving to Step 03 - Traditional ML Baseline Models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
