{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "153aa10e",
   "metadata": {},
   "source": [
    "# ðŸ”§ Step 2: Data Preprocessing and Feature Engineering\n",
    "## Advanced Data Transformation for Sepsis Prediction\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ **Objectives**\n",
    "- **Data Cleaning**: Handle missing values with clinical-informed strategies\n",
    "- **Feature Engineering**: Create temporal and clinical domain features\n",
    "- **Data Normalization**: Standardize features for machine learning\n",
    "- **Quality Assurance**: Validate preprocessing pipeline integrity\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ¥ **Clinical-Informed Preprocessing Strategy**\n",
    "\n",
    "#### **Missing Value Handling** ðŸ©º\n",
    "| **Clinical Context** | **Imputation Strategy** | **Rationale** |\n",
    "|---------------------|------------------------|---------------|\n",
    "| **Vital Signs** | Forward-fill + Clinical bounds | Maintains physiological continuity |\n",
    "| **Lab Values** | Median + Time-decay | Reflects testing frequency patterns |\n",
    "| **Blood Gas** | Interpolation | Captures respiratory dynamics |\n",
    "| **Demographics** | Mode/Median | Stable patient characteristics |\n",
    "\n",
    "#### **Feature Engineering Categories** ðŸ§¬\n",
    "1. **Temporal Features**: Trends, slopes, variability measures\n",
    "2. **Clinical Ratios**: Shock index, oxygen ratios, perfusion indicators\n",
    "3. **Statistical Features**: Rolling statistics, percentiles, outlier indicators\n",
    "4. **Time-Since Features**: Time since abnormal values, admission time\n",
    "5. **Interaction Features**: Multi-organ system interactions\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¬ **Advanced Feature Engineering**\n",
    "\n",
    "#### **Temporal Dynamics** â±ï¸\n",
    "- **Trend Analysis**: Slope calculations over sliding windows\n",
    "- **Variability Metrics**: Standard deviation, coefficient of variation\n",
    "- **Change Point Detection**: Sudden physiological changes\n",
    "- **Time-Series Decomposition**: Trend, seasonality, residuals\n",
    "\n",
    "#### **Clinical Scoring Systems** ðŸ“Š\n",
    "- **SOFA-inspired Features**: Organ dysfunction indicators\n",
    "- **NEWS-based Features**: Early warning score components\n",
    "- **Custom Sepsis Indicators**: Domain-specific risk markers\n",
    "- **Multi-organ Integration**: Cross-system interaction patterns\n",
    "\n",
    "#### **Statistical Transformations** ðŸ“ˆ\n",
    "- **Normalization**: Z-score, Min-Max, Robust scaling\n",
    "- **Distribution Adjustment**: Log, Box-Cox transformations\n",
    "- **Outlier Handling**: Clinical bounds, statistical methods\n",
    "- **Feature Scaling**: Unit standardization, clinical range mapping\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“‹ **Pipeline Architecture**\n",
    "1. **Data Loading & Validation**\n",
    "2. **Missing Value Analysis & Imputation**\n",
    "3. **Temporal Feature Engineering**\n",
    "4. **Clinical Feature Creation**\n",
    "5. **Statistical Transformations**\n",
    "6. **Feature Selection & Validation**\n",
    "7. **Data Export for Modeling**\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ **Expected Outputs**\n",
    "- Clean, imputed dataset ready for modeling\n",
    "- Rich feature set with temporal and clinical insights\n",
    "- Preprocessing pipeline for production deployment\n",
    "- Feature importance and correlation analysis\n",
    "- Data quality validation reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a394a243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy import stats\n",
    "from scipy.interpolate import interp1d\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be1172a",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Environment Setup and Data Loading\n",
    "\n",
    "Initializing the preprocessing environment with advanced libraries for clinical data transformation and feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff6ce646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: C:\\Users\\sachi\\Desktop\\Sepsis STFT\\data\\raw\\training_setA (1)\n",
      "Processed data will be saved to: C:\\Users\\sachi\\Desktop\\Sepsis STFT\\data\\processed\n",
      "Models will be saved to: C:\\Users\\sachi\\Desktop\\Sepsis STFT\\models\n"
     ]
    }
   ],
   "source": [
    "# Configuration and paths\n",
    "DATA_PATH = r\"C:\\Users\\sachi\\Desktop\\Sepsis STFT\\data\\raw\\training_setA (1)\"\n",
    "PROCESSED_DATA_PATH = r\"C:\\Users\\sachi\\Desktop\\Sepsis STFT\\data\\processed\"\n",
    "MODEL_PATH = r\"C:\\Users\\sachi\\Desktop\\Sepsis STFT\\models\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Processed data will be saved to: {PROCESSED_DATA_PATH}\")\n",
    "print(f\"Models will be saved to: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c044f9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data subset for preprocessing development...\n",
      "Loading 1000 patient files...\n",
      "Loaded 0/1000 files...\n",
      "Successfully loaded 1000 files\n",
      "Combined dataset shape: (38809, 42)\n",
      "Data loaded successfully: (38809, 42)\n",
      "Unique patients: 1000\n"
     ]
    }
   ],
   "source": [
    "# Data loading functions\n",
    "def load_psv_file(filepath):\n",
    "    \"\"\"Load a single PSV file and add patient ID\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, sep='|')\n",
    "        patient_id = os.path.basename(filepath).replace('.psv', '')\n",
    "        df['PatientID'] = patient_id\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_all_data(data_path, max_patients=None):\n",
    "    \"\"\"Load all PSV files and combine them\"\"\"\n",
    "    psv_files = glob.glob(os.path.join(data_path, \"*.psv\"))\n",
    "    \n",
    "    if max_patients:\n",
    "        psv_files = psv_files[:max_patients]\n",
    "    \n",
    "    print(f\"Loading {len(psv_files)} patient files...\")\n",
    "    \n",
    "    data_list = []\n",
    "    failed_files = []\n",
    "    \n",
    "    for i, file in enumerate(psv_files):\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Loaded {i}/{len(psv_files)} files...\")\n",
    "        \n",
    "        df = load_psv_file(file)\n",
    "        if df is not None:\n",
    "            data_list.append(df)\n",
    "        else:\n",
    "            failed_files.append(file)\n",
    "    \n",
    "    if failed_files:\n",
    "        print(f\"Failed to load {len(failed_files)} files\")\n",
    "    \n",
    "    if data_list:\n",
    "        combined_data = pd.concat(data_list, ignore_index=True)\n",
    "        print(f\"Successfully loaded {len(data_list)} files\")\n",
    "        print(f\"Combined dataset shape: {combined_data.shape}\")\n",
    "        return combined_data\n",
    "    else:\n",
    "        raise ValueError(\"No data loaded successfully\")\n",
    "\n",
    "# Load subset for development (first 1000 patients)\n",
    "print(\"Loading data subset for preprocessing development...\")\n",
    "data = load_all_data(DATA_PATH, max_patients=1000)\n",
    "print(f\"Data loaded successfully: {data.shape}\")\n",
    "print(f\"Unique patients: {data['PatientID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff701154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature groups defined:\n",
      "- Vital signs: 7\n",
      "- Lab values: 20\n",
      "- Gas analysis: 7\n",
      "- Demographics: 2\n",
      "- Clinical context: 4\n"
     ]
    }
   ],
   "source": [
    "# Define feature groups for medical domain knowledge\n",
    "VITAL_SIGNS = ['HR', 'O2Sat', 'Temp', 'SBP', 'MAP', 'DBP', 'Resp']\n",
    "LAB_VALUES = ['AST', 'BUN', 'Alkalinephos', 'Calcium', 'Chloride', 'Creatinine',\n",
    "              'Bilirubin_direct', 'Glucose', 'Lactate', 'Magnesium', 'Phosphate',\n",
    "              'Potassium', 'Bilirubin_total', 'TroponinI', 'Hct', 'Hgb', 'PTT',\n",
    "              'WBC', 'Fibrinogen', 'Platelets']\n",
    "GAS_ANALYSIS = ['EtCO2', 'BaseExcess', 'HCO3', 'FiO2', 'pH', 'PaCO2', 'SaO2']\n",
    "DEMOGRAPHICS = ['Age', 'Gender']\n",
    "CLINICAL_CONTEXT = ['Unit1', 'Unit2', 'HospAdmTime', 'ICULOS']\n",
    "TARGET = ['SepsisLabel']\n",
    "\n",
    "# Medical reference ranges for outlier detection\n",
    "MEDICAL_RANGES = {\n",
    "    'HR': (30, 200),\n",
    "    'O2Sat': (70, 100),\n",
    "    'Temp': (30, 45),\n",
    "    'SBP': (50, 300),\n",
    "    'MAP': (30, 200),\n",
    "    'DBP': (20, 150),\n",
    "    'Resp': (5, 50),\n",
    "    'Age': (0, 120),\n",
    "    'pH': (6.5, 8.0),\n",
    "    'Glucose': (20, 800)\n",
    "}\n",
    "\n",
    "# Normal values for medical imputation\n",
    "NORMAL_VALUES = {\n",
    "    'HR': 80,\n",
    "    'O2Sat': 98,\n",
    "    'Temp': 36.5,\n",
    "    'SBP': 120,\n",
    "    'MAP': 90,\n",
    "    'DBP': 80,\n",
    "    'Resp': 16,\n",
    "    'pH': 7.4,\n",
    "    'FiO2': 0.21\n",
    "}\n",
    "\n",
    "print(f\"Feature groups defined:\")\n",
    "print(f\"- Vital signs: {len(VITAL_SIGNS)}\")\n",
    "print(f\"- Lab values: {len(LAB_VALUES)}\")\n",
    "print(f\"- Gas analysis: {len(GAS_ANALYSIS)}\")\n",
    "print(f\"- Demographics: {len(DEMOGRAPHICS)}\")\n",
    "print(f\"- Clinical context: {len(CLINICAL_CONTEXT)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf2998e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA QUALITY ASSESSMENT ===\n",
      "\n",
      "Missing data summary:\n",
      "Features with >50% missing: 28\n",
      "EtCO2               100.00\n",
      "TroponinI            99.84\n",
      "Bilirubin_direct     99.84\n",
      "Fibrinogen           99.15\n",
      "Bilirubin_total      98.86\n",
      "Alkalinephos         98.54\n",
      "AST                  98.50\n",
      "Lactate              96.40\n",
      "Calcium              95.32\n",
      "PTT                  95.31\n",
      "dtype: float64\n",
      "\n",
      "Duplicate rows: 0\n",
      "\n",
      "Data types:\n",
      "float64    38\n",
      "int64       3\n",
      "object      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Data quality assessment and cleaning\n",
    "def assess_data_quality(df):\n",
    "    \"\"\"Assess and report data quality issues\"\"\"\n",
    "    print(\"=== DATA QUALITY ASSESSMENT ===\")\n",
    "    \n",
    "    # Missing data analysis\n",
    "    missing_data = df.isnull().sum().sort_values(ascending=False)\n",
    "    missing_percent = (missing_data / len(df) * 100).round(2)\n",
    "    \n",
    "    print(f\"\\nMissing data summary:\")\n",
    "    high_missing = missing_percent[missing_percent > 50]\n",
    "    if len(high_missing) > 0:\n",
    "        print(f\"Features with >50% missing: {len(high_missing)}\")\n",
    "        print(high_missing.head(10))\n",
    "    \n",
    "    # Duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\nDuplicate rows: {duplicates}\")\n",
    "    \n",
    "    # Data types\n",
    "    print(f\"\\nData types:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    return missing_data, missing_percent\n",
    "\n",
    "missing_data, missing_percent = assess_data_quality(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e762d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OUTLIER DETECTION ===\n",
      "O2Sat: 22 outliers outside [70, 100]\n",
      "Temp: 2 outliers outside [30, 45]\n",
      "SBP: 5 outliers outside [50, 300]\n",
      "MAP: 17 outliers outside [30, 200]\n",
      "DBP: 5 outliers outside [20, 150]\n",
      "Resp: 34 outliers outside [5, 50]\n",
      "Glucose: 3 outliers outside [20, 800]\n",
      "\n",
      "Data shape after outlier treatment: (38809, 42)\n"
     ]
    }
   ],
   "source": [
    "# Outlier detection and treatment using medical knowledge\n",
    "def detect_medical_outliers(df, feature_ranges):\n",
    "    \"\"\"Detect outliers using medical reference ranges\"\"\"\n",
    "    outlier_counts = {}\n",
    "    \n",
    "    for feature, (min_val, max_val) in feature_ranges.items():\n",
    "        if feature in df.columns:\n",
    "            outliers = ((df[feature] < min_val) | (df[feature] > max_val))\n",
    "            outlier_count = outliers.sum()\n",
    "            outlier_counts[feature] = outlier_count\n",
    "            \n",
    "            if outlier_count > 0:\n",
    "                print(f\"{feature}: {outlier_count} outliers outside [{min_val}, {max_val}]\")\n",
    "    \n",
    "    return outlier_counts\n",
    "\n",
    "def treat_outliers(df, feature_ranges, method='clip'):\n",
    "    \"\"\"Treat outliers using medical reference ranges\"\"\"\n",
    "    df_treated = df.copy()\n",
    "    \n",
    "    for feature, (min_val, max_val) in feature_ranges.items():\n",
    "        if feature in df_treated.columns:\n",
    "            if method == 'clip':\n",
    "                df_treated[feature] = df_treated[feature].clip(min_val, max_val)\n",
    "            elif method == 'remove':\n",
    "                mask = (df_treated[feature] >= min_val) & (df_treated[feature] <= max_val)\n",
    "                df_treated = df_treated[mask]\n",
    "    \n",
    "    return df_treated\n",
    "\n",
    "# Detect outliers\n",
    "print(\"=== OUTLIER DETECTION ===\")\n",
    "outlier_counts = detect_medical_outliers(data, MEDICAL_RANGES)\n",
    "\n",
    "# Treat outliers by clipping to medical ranges\n",
    "data_cleaned = treat_outliers(data, MEDICAL_RANGES, method='clip')\n",
    "print(f\"\\nData shape after outlier treatment: {data_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "702211ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MISSING VALUE IMPUTATION ===\n",
      "Before imputation - Missing values: 1086709\n",
      "After imputation - Missing values: 38809\n",
      "Final missing values: 38809\n"
     ]
    }
   ],
   "source": [
    "# Missing value imputation with medical domain knowledge\n",
    "class MedicalImputer:\n",
    "    \"\"\"Custom imputer for medical time series data\"\"\"\n",
    "    \n",
    "    def __init__(self, strategy='medical_forward_fill'):\n",
    "        self.strategy = strategy\n",
    "        self.imputation_values = {}\n",
    "    \n",
    "    def fit(self, df):\n",
    "        \"\"\"Fit the imputer to the data\"\"\"\n",
    "        # Calculate median values for each feature\n",
    "        for col in df.columns:\n",
    "            if col not in ['PatientID', 'SepsisLabel', 'ICULOS', 'HospAdmTime']:\n",
    "                if col in NORMAL_VALUES:\n",
    "                    self.imputation_values[col] = NORMAL_VALUES[col]\n",
    "                else:\n",
    "                    self.imputation_values[col] = df[col].median()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \"\"\"Apply imputation to the data\"\"\"\n",
    "        df_imputed = df.copy()\n",
    "        \n",
    "        # Sort by patient and time for forward fill\n",
    "        df_imputed = df_imputed.sort_values(['PatientID', 'ICULOS'])\n",
    "        \n",
    "        if self.strategy == 'medical_forward_fill':\n",
    "            # Forward fill within each patient\n",
    "            for patient_id in df_imputed['PatientID'].unique():\n",
    "                patient_mask = df_imputed['PatientID'] == patient_id\n",
    "                \n",
    "                # Forward fill for each feature group\n",
    "                for feature_group in [VITAL_SIGNS, LAB_VALUES, GAS_ANALYSIS]:\n",
    "                    available_features = [f for f in feature_group if f in df_imputed.columns]\n",
    "                    df_imputed.loc[patient_mask, available_features] = df_imputed.loc[patient_mask, available_features].fillna(method='ffill')\n",
    "                \n",
    "                # Fill remaining missing values with medical normal values or median\n",
    "                for col in df_imputed.columns:\n",
    "                    if col in self.imputation_values:\n",
    "                        df_imputed.loc[patient_mask, col] = df_imputed.loc[patient_mask, col].fillna(self.imputation_values[col])\n",
    "        \n",
    "        return df_imputed\n",
    "    \n",
    "    def fit_transform(self, df):\n",
    "        return self.fit(df).transform(df)\n",
    "\n",
    "# Apply medical imputation\n",
    "print(\"=== MISSING VALUE IMPUTATION ===\")\n",
    "print(f\"Before imputation - Missing values: {data_cleaned.isnull().sum().sum()}\")\n",
    "\n",
    "imputer = MedicalImputer(strategy='medical_forward_fill')\n",
    "data_imputed = imputer.fit_transform(data_cleaned)\n",
    "\n",
    "print(f\"After imputation - Missing values: {data_imputed.isnull().sum().sum()}\")\n",
    "\n",
    "# Fill any remaining missing values with median\n",
    "for col in data_imputed.columns:\n",
    "    if data_imputed[col].isnull().any() and col not in ['PatientID']:\n",
    "        if data_imputed[col].dtype in ['float64', 'int64']:\n",
    "            data_imputed[col].fillna(data_imputed[col].median(), inplace=True)\n",
    "        else:\n",
    "            data_imputed[col].fillna(data_imputed[col].mode()[0], inplace=True)\n",
    "\n",
    "print(f\"Final missing values: {data_imputed.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f1daa75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporal features...\n",
      "Creating statistical features...\n",
      "Created temporal features. New shape: (38809, 122)\n",
      "\n",
      "Feature engineering completed. Shape: (38809, 122)\n",
      "New features created: 80\n"
     ]
    }
   ],
   "source": [
    "# Feature engineering for temporal patterns\n",
    "def create_temporal_features(df):\n",
    "    \"\"\"Create temporal features for time series analysis\"\"\"\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Sort by patient and time\n",
    "    df_features = df_features.sort_values(['PatientID', 'ICULOS'])\n",
    "    \n",
    "    print(\"Creating temporal features...\")\n",
    "    \n",
    "    # Time-based features\n",
    "    df_features['Hour_in_ICU'] = df_features['ICULOS']\n",
    "    df_features['Time_since_admission'] = df_features['ICULOS'] + df_features['HospAdmTime']\n",
    "    \n",
    "    # Cyclical time features\n",
    "    df_features['Hour_sin'] = np.sin(2 * np.pi * df_features['ICULOS'] / 24)\n",
    "    df_features['Hour_cos'] = np.cos(2 * np.pi * df_features['ICULOS'] / 24)\n",
    "    \n",
    "    # Patient-level aggregated features\n",
    "    for patient_id in df_features['PatientID'].unique():\n",
    "        patient_mask = df_features['PatientID'] == patient_id\n",
    "        patient_data = df_features[patient_mask].copy()\n",
    "        \n",
    "        # Rolling window features (3-hour and 6-hour windows)\n",
    "        for window in [3, 6]:\n",
    "            for feature in VITAL_SIGNS + ['Lactate', 'WBC', 'Glucose']:\n",
    "                if feature in df_features.columns:\n",
    "                    # Rolling mean\n",
    "                    rolling_mean = patient_data[feature].rolling(window=window, min_periods=1).mean()\n",
    "                    df_features.loc[patient_mask, f'{feature}_rolling_mean_{window}h'] = rolling_mean\n",
    "                    \n",
    "                    # Rolling std\n",
    "                    rolling_std = patient_data[feature].rolling(window=window, min_periods=1).std()\n",
    "                    df_features.loc[patient_mask, f'{feature}_rolling_std_{window}h'] = rolling_std.fillna(0)\n",
    "                    \n",
    "                    # Rolling max/min\n",
    "                    rolling_max = patient_data[feature].rolling(window=window, min_periods=1).max()\n",
    "                    rolling_min = patient_data[feature].rolling(window=window, min_periods=1).min()\n",
    "                    df_features.loc[patient_mask, f'{feature}_rolling_range_{window}h'] = rolling_max - rolling_min\n",
    "        \n",
    "        # Trend features (slope over last 3 hours)\n",
    "        for feature in VITAL_SIGNS + ['Lactate', 'WBC']:\n",
    "            if feature in df_features.columns:\n",
    "                # Calculate slope using linear regression over rolling window\n",
    "                trends = []\n",
    "                for i in range(len(patient_data)):\n",
    "                    start_idx = max(0, i-2)  # 3-hour window\n",
    "                    y_vals = patient_data[feature].iloc[start_idx:i+1].values\n",
    "                    x_vals = np.arange(len(y_vals))\n",
    "                    \n",
    "                    if len(y_vals) > 1:\n",
    "                        slope, _, _, _, _ = stats.linregress(x_vals, y_vals)\n",
    "                        trends.append(slope)\n",
    "                    else:\n",
    "                        trends.append(0)\n",
    "                \n",
    "                df_features.loc[patient_mask, f'{feature}_trend_3h'] = trends\n",
    "    \n",
    "    # Statistical features\n",
    "    print(\"Creating statistical features...\")\n",
    "    \n",
    "    # SOFA-like composite scores\n",
    "    # Cardiovascular SOFA component\n",
    "    df_features['Cardiovascular_score'] = 0\n",
    "    df_features.loc[df_features['MAP'] < 70, 'Cardiovascular_score'] = 1\n",
    "    df_features.loc[df_features['MAP'] < 60, 'Cardiovascular_score'] = 2\n",
    "    \n",
    "    # Respiratory SOFA component\n",
    "    df_features['Respiratory_score'] = 0\n",
    "    pf_ratio = df_features['O2Sat'] / (df_features['FiO2'] + 0.01)  # Approximate P/F ratio\n",
    "    df_features.loc[pf_ratio < 400, 'Respiratory_score'] = 1\n",
    "    df_features.loc[pf_ratio < 300, 'Respiratory_score'] = 2\n",
    "    df_features.loc[pf_ratio < 200, 'Respiratory_score'] = 3\n",
    "    \n",
    "    # Renal SOFA component\n",
    "    df_features['Renal_score'] = 0\n",
    "    df_features.loc[df_features['Creatinine'] > 1.2, 'Renal_score'] = 1\n",
    "    df_features.loc[df_features['Creatinine'] > 2.0, 'Renal_score'] = 2\n",
    "    df_features.loc[df_features['Creatinine'] > 3.5, 'Renal_score'] = 3\n",
    "    \n",
    "    # Combine scores\n",
    "    df_features['Total_SOFA_approx'] = (df_features['Cardiovascular_score'] + \n",
    "                                        df_features['Respiratory_score'] + \n",
    "                                        df_features['Renal_score'])\n",
    "    \n",
    "    # Additional medical ratios and indices\n",
    "    df_features['Shock_index'] = df_features['HR'] / (df_features['SBP'] + 0.1)\n",
    "    df_features['Modified_shock_index'] = df_features['HR'] / (df_features['MAP'] + 0.1)\n",
    "    df_features['Oxygen_index'] = df_features['O2Sat'] / (df_features['FiO2'] + 0.01)\n",
    "    \n",
    "    print(f\"Created temporal features. New shape: {df_features.shape}\")\n",
    "    return df_features\n",
    "\n",
    "# Create temporal features\n",
    "data_with_features = create_temporal_features(data_imputed)\n",
    "print(f\"\\nFeature engineering completed. Shape: {data_with_features.shape}\")\n",
    "print(f\"New features created: {data_with_features.shape[1] - data_imputed.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cb4c206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling preparation:\n",
      "- Robust scaling: 96 features\n",
      "- Standard scaling: 8 features\n",
      "- MinMax scaling: 7 features\n",
      "\n",
      "Data scaling completed. Shape: (38809, 122)\n"
     ]
    }
   ],
   "source": [
    "# Data scaling and normalization\n",
    "def prepare_scaling(df, feature_groups):\n",
    "    \"\"\"Prepare data for scaling by feature groups\"\"\"\n",
    "    scalers = {}\n",
    "    \n",
    "    # Separate features that need different scaling\n",
    "    robust_features = []  # For features with outliers\n",
    "    standard_features = []  # For normally distributed features\n",
    "    minmax_features = []  # For bounded features\n",
    "    \n",
    "    for feature_group_name, features in feature_groups.items():\n",
    "        available_features = [f for f in features if f in df.columns]\n",
    "        \n",
    "        if feature_group_name in ['VITAL_SIGNS', 'LAB_VALUES']:\n",
    "            robust_features.extend(available_features)\n",
    "        elif feature_group_name in ['DEMOGRAPHICS']:\n",
    "            standard_features.extend(available_features)\n",
    "        else:\n",
    "            minmax_features.extend(available_features)\n",
    "    \n",
    "    # Add rolling features to robust scaling\n",
    "    rolling_features = [col for col in df.columns if 'rolling' in col or 'trend' in col]\n",
    "    robust_features.extend(rolling_features)\n",
    "    \n",
    "    # Add composite scores to standard scaling\n",
    "    score_features = [col for col in df.columns if 'score' in col or 'index' in col]\n",
    "    standard_features.extend(score_features)\n",
    "    \n",
    "    return robust_features, standard_features, minmax_features\n",
    "\n",
    "# Prepare feature groups for scaling\n",
    "feature_groups = {\n",
    "    'VITAL_SIGNS': VITAL_SIGNS,\n",
    "    'LAB_VALUES': LAB_VALUES,\n",
    "    'GAS_ANALYSIS': GAS_ANALYSIS,\n",
    "    'DEMOGRAPHICS': DEMOGRAPHICS\n",
    "}\n",
    "\n",
    "robust_features, standard_features, minmax_features = prepare_scaling(data_with_features, feature_groups)\n",
    "\n",
    "print(f\"Scaling preparation:\")\n",
    "print(f\"- Robust scaling: {len(robust_features)} features\")\n",
    "print(f\"- Standard scaling: {len(standard_features)} features\")\n",
    "print(f\"- MinMax scaling: {len(minmax_features)} features\")\n",
    "\n",
    "# Create and fit scalers\n",
    "scalers = {}\n",
    "data_scaled = data_with_features.copy()\n",
    "\n",
    "if robust_features:\n",
    "    scalers['robust'] = RobustScaler()\n",
    "    data_scaled[robust_features] = scalers['robust'].fit_transform(data_scaled[robust_features])\n",
    "\n",
    "if standard_features:\n",
    "    scalers['standard'] = StandardScaler()\n",
    "    data_scaled[standard_features] = scalers['standard'].fit_transform(data_scaled[standard_features])\n",
    "\n",
    "if minmax_features:\n",
    "    scalers['minmax'] = MinMaxScaler()\n",
    "    data_scaled[minmax_features] = scalers['minmax'].fit_transform(data_scaled[minmax_features])\n",
    "\n",
    "print(f\"\\nData scaling completed. Shape: {data_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dde5adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CREATING ML DATASETS ===\n",
      "Creating ML dataset with sequence length: 12 hours\n",
      "Created 26863 sequences\n",
      "Sequence shape: (26863, 12, 117)\n",
      "Target distribution: [26207   656]\n",
      "\n",
      "Flattened dataset:\n",
      "X shape: (38809, 116)\n",
      "y distribution: [37945   864]\n"
     ]
    }
   ],
   "source": [
    "# Temporal data preparation for machine learning\n",
    "def create_ml_dataset(df, sequence_length=12, prediction_horizon=1):\n",
    "    \"\"\"Create ML dataset with sequences for temporal modeling\"\"\"\n",
    "    print(f\"Creating ML dataset with sequence length: {sequence_length} hours\")\n",
    "    \n",
    "    # Exclude non-feature columns\n",
    "    exclude_cols = ['PatientID', 'ICULOS', 'HospAdmTime', 'Unit1', 'Unit2']\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "    patient_ids = []\n",
    "    timestamps = []\n",
    "    \n",
    "    for patient_id in df['PatientID'].unique():\n",
    "        patient_data = df[df['PatientID'] == patient_id].sort_values('ICULOS')\n",
    "        \n",
    "        if len(patient_data) >= sequence_length + prediction_horizon:\n",
    "            for i in range(len(patient_data) - sequence_length - prediction_horizon + 1):\n",
    "                # Input sequence\n",
    "                sequence_data = patient_data.iloc[i:i+sequence_length]\n",
    "                X_sequence = sequence_data[feature_cols].values\n",
    "                \n",
    "                # Target (predict sepsis at prediction_horizon hours ahead)\n",
    "                target_idx = i + sequence_length + prediction_horizon - 1\n",
    "                y_target = patient_data.iloc[target_idx]['SepsisLabel']\n",
    "                \n",
    "                X_sequences.append(X_sequence)\n",
    "                y_sequences.append(y_target)\n",
    "                patient_ids.append(patient_id)\n",
    "                timestamps.append(patient_data.iloc[target_idx]['ICULOS'])\n",
    "    \n",
    "    X = np.array(X_sequences)\n",
    "    y = np.array(y_sequences)\n",
    "    \n",
    "    print(f\"Created {len(X)} sequences\")\n",
    "    print(f\"Sequence shape: {X.shape}\")\n",
    "    print(f\"Target distribution: {np.bincount(y)}\")\n",
    "    \n",
    "    return X, y, np.array(patient_ids), np.array(timestamps), feature_cols\n",
    "\n",
    "# Create both sequence and flattened datasets\n",
    "print(\"=== CREATING ML DATASETS ===\")\n",
    "\n",
    "# Sequence dataset for LSTM/RNN models\n",
    "X_seq, y_seq, patient_ids_seq, timestamps_seq, feature_names = create_ml_dataset(data_scaled, sequence_length=12)\n",
    "\n",
    "# Flattened dataset for traditional ML models\n",
    "exclude_cols = ['PatientID', 'ICULOS', 'HospAdmTime', 'Unit1', 'Unit2']\n",
    "feature_columns = [col for col in data_scaled.columns if col not in exclude_cols and col != 'SepsisLabel']\n",
    "\n",
    "X_flat = data_scaled[feature_columns].values\n",
    "y_flat = data_scaled['SepsisLabel'].values\n",
    "\n",
    "print(f\"\\nFlattened dataset:\")\n",
    "print(f\"X shape: {X_flat.shape}\")\n",
    "print(f\"y distribution: {np.bincount(y_flat)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d437203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEMPORAL TRAIN-VALIDATION-TEST SPLIT ===\n",
      "Split summary:\n",
      "- Train patients: 700 (70.0%)\n",
      "- Validation patients: 100 (10.0%)\n",
      "- Test patients: 200 (20.0%)\n",
      "\n",
      "Data distribution:\n",
      "- Train samples: 26714\n",
      "- Validation samples: 3923\n",
      "- Test samples: 8172\n",
      "\n",
      "Sepsis distribution:\n",
      "- Train: 2.27% sepsis cases\n",
      "- Validation: 2.22% sepsis cases\n",
      "- Test: 2.08% sepsis cases\n"
     ]
    }
   ],
   "source": [
    "# Temporal train-test split\n",
    "def temporal_train_test_split(df, test_size=0.2, validation_size=0.1):\n",
    "    \"\"\"Split data temporally by patients to avoid data leakage\"\"\"\n",
    "    # Get unique patients and their sepsis status\n",
    "    patient_info = df.groupby('PatientID').agg({\n",
    "        'SepsisLabel': 'max',\n",
    "        'ICULOS': 'max'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Stratified split by sepsis status\n",
    "    sepsis_patients = patient_info[patient_info['SepsisLabel'] == 1]['PatientID'].values\n",
    "    no_sepsis_patients = patient_info[patient_info['SepsisLabel'] == 0]['PatientID'].values\n",
    "    \n",
    "    # Split each group\n",
    "    np.random.shuffle(sepsis_patients)\n",
    "    np.random.shuffle(no_sepsis_patients)\n",
    "    \n",
    "    # Calculate split indices\n",
    "    n_sepsis_test = int(len(sepsis_patients) * test_size)\n",
    "    n_sepsis_val = int(len(sepsis_patients) * validation_size)\n",
    "    \n",
    "    n_no_sepsis_test = int(len(no_sepsis_patients) * test_size)\n",
    "    n_no_sepsis_val = int(len(no_sepsis_patients) * validation_size)\n",
    "    \n",
    "    # Create splits\n",
    "    test_patients = np.concatenate([\n",
    "        sepsis_patients[:n_sepsis_test],\n",
    "        no_sepsis_patients[:n_no_sepsis_test]\n",
    "    ])\n",
    "    \n",
    "    val_patients = np.concatenate([\n",
    "        sepsis_patients[n_sepsis_test:n_sepsis_test + n_sepsis_val],\n",
    "        no_sepsis_patients[n_no_sepsis_test:n_no_sepsis_test + n_no_sepsis_val]\n",
    "    ])\n",
    "    \n",
    "    train_patients = np.concatenate([\n",
    "        sepsis_patients[n_sepsis_test + n_sepsis_val:],\n",
    "        no_sepsis_patients[n_no_sepsis_test + n_no_sepsis_val:]\n",
    "    ])\n",
    "    \n",
    "    # Create boolean masks\n",
    "    train_mask = df['PatientID'].isin(train_patients)\n",
    "    val_mask = df['PatientID'].isin(val_patients)\n",
    "    test_mask = df['PatientID'].isin(test_patients)\n",
    "    \n",
    "    return train_mask, val_mask, test_mask, train_patients, val_patients, test_patients\n",
    "\n",
    "# Create temporal splits\n",
    "print(\"=== TEMPORAL TRAIN-VALIDATION-TEST SPLIT ===\")\n",
    "train_mask, val_mask, test_mask, train_patients, val_patients, test_patients = temporal_train_test_split(data_scaled)\n",
    "\n",
    "print(f\"Split summary:\")\n",
    "print(f\"- Train patients: {len(train_patients)} ({len(train_patients)/data_scaled['PatientID'].nunique()*100:.1f}%)\")\n",
    "print(f\"- Validation patients: {len(val_patients)} ({len(val_patients)/data_scaled['PatientID'].nunique()*100:.1f}%)\")\n",
    "print(f\"- Test patients: {len(test_patients)} ({len(test_patients)/data_scaled['PatientID'].nunique()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nData distribution:\")\n",
    "print(f\"- Train samples: {train_mask.sum()}\")\n",
    "print(f\"- Validation samples: {val_mask.sum()}\")\n",
    "print(f\"- Test samples: {test_mask.sum()}\")\n",
    "\n",
    "# Check sepsis distribution in each split\n",
    "print(f\"\\nSepsis distribution:\")\n",
    "for split_name, mask in [('Train', train_mask), ('Validation', val_mask), ('Test', test_mask)]:\n",
    "    split_data = data_scaled[mask]\n",
    "    sepsis_rate = split_data['SepsisLabel'].mean() * 100\n",
    "    print(f\"- {split_name}: {sepsis_rate:.2f}% sepsis cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09b9213a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 02 COMPLETED ===\n",
      "âœ“ Data loading and preprocessing pipeline established\n",
      "âœ“ Missing value handling implemented\n",
      "âœ“ Feature engineering completed\n",
      "âœ“ Data scaling and normalization applied\n",
      "âœ“ Train/validation/test splits created\n",
      "\n",
      "Step 02 completed successfully!\n",
      "Moving to Step 03 - Traditional ML Baseline Models\n"
     ]
    }
   ],
   "source": [
    "# Complete Step 02 - Data Preprocessing\n",
    "print(\"=== STEP 02 COMPLETED ===\")\n",
    "print(\"âœ“ Data loading and preprocessing pipeline established\")\n",
    "print(\"âœ“ Missing value handling implemented\")\n",
    "print(\"âœ“ Feature engineering completed\")\n",
    "print(\"âœ“ Data scaling and normalization applied\")\n",
    "print(\"âœ“ Train/validation/test splits created\")\n",
    "print(\"\\nStep 02 completed successfully!\")\n",
    "print(\"Moving to Step 03 - Traditional ML Baseline Models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
