{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bec47ea",
   "metadata": {},
   "source": [
    "# Comprehensive Validation and Testing for Sepsis Prediction Models\n",
    "\n",
    "This notebook implements comprehensive validation strategies and testing methodologies to ensure robust and clinically relevant sepsis prediction models.\n",
    "\n",
    "## Validation Strategies\n",
    "1. **Temporal Validation**: Time-based data splits to test model performance over time\n",
    "2. **Clinical Performance Metrics**: Healthcare-specific evaluation metrics\n",
    "3. **Feature Importance Aggregation**: Robust feature importance across multiple models\n",
    "4. **Model Interpretability**: SHAP values and LIME explanations\n",
    "5. **Bias and Fairness Evaluation**: Performance across different demographic groups\n",
    "6. **Subgroup Analysis**: Performance across different patient populations\n",
    "\n",
    "## Testing Framework\n",
    "- **Robustness Testing**: Model performance under different conditions\n",
    "- **Stability Analysis**: Consistency across multiple random seeds\n",
    "- **Clinical Validation**: Alignment with medical knowledge and guidelines\n",
    "- **Edge Case Testing**: Performance on rare but critical cases\n",
    "- **Uncertainty Quantification**: Model confidence and prediction reliability\n",
    "\n",
    "## Clinical Validation Metrics\n",
    "- Sensitivity (Recall) for sepsis detection\n",
    "- Specificity to minimize false alarms\n",
    "- Positive/Negative Predictive Values\n",
    "- Early detection capability\n",
    "- Clinical decision support integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0e97bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIME not available. Install with: pip install lime\n",
      "AIF360 not available. Install for fairness analysis.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "# Machine learning and validation\n",
    "from sklearn.model_selection import (\n",
    "    TimeSeriesSplit, StratifiedKFold, LeaveOneGroupOut,\n",
    "    cross_val_score, cross_validate, validation_curve\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve,\n",
    "    confusion_matrix, classification_report,\n",
    "    average_precision_score, matthews_corrcoef,\n",
    "    cohen_kappa_score, brier_score_loss\n",
    ")\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Interpretability and explainability\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"SHAP not available. Install with: pip install shap\")\n",
    "    SHAP_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import lime\n",
    "    import lime.lime_tabular\n",
    "    LIME_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"LIME not available. Install with: pip install lime\")\n",
    "    LIME_AVAILABLE = False\n",
    "\n",
    "# Statistical tests\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, mannwhitneyu, ttest_ind\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "# Plotting and visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Fairness evaluation\n",
    "try:\n",
    "    from aif360.datasets import BinaryLabelDataset\n",
    "    from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "    AIF360_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"AIF360 not available. Install for fairness analysis.\")\n",
    "    AIF360_AVAILABLE = False\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6e11d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation configuration initialized!\n",
      "Results will be saved to: results/validation/\n"
     ]
    }
   ],
   "source": [
    "# Configuration for validation and testing\n",
    "class ValidationConfig:\n",
    "    DATA_PATH = \"data/processed/\"\n",
    "    MODELS_PATH = \"models/\"\n",
    "    RESULTS_PATH = \"results/validation/\"\n",
    "    PLOTS_PATH = \"plots/validation/\"\n",
    "    \n",
    "    # Create directories\n",
    "    for path in [RESULTS_PATH, PLOTS_PATH]:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    # Validation parameters\n",
    "    RANDOM_STATE = 42\n",
    "    N_BOOTSTRAP = 1000\n",
    "    CONFIDENCE_LEVEL = 0.95\n",
    "    TIME_SPLITS = 5\n",
    "    \n",
    "    # Clinical thresholds\n",
    "    SEPSIS_ONSET_HOURS = 6  # Early detection threshold\n",
    "    ICU_MORTALITY_THRESHOLD = 0.1  # 10% mortality threshold\n",
    "    \n",
    "    # Subgroup analysis\n",
    "    SUBGROUPS = {\n",
    "        'age': [18, 65, 80],  # Young adults, elderly, very elderly\n",
    "        'gender': ['M', 'F'],\n",
    "        'icu_unit': ['MICU', 'SICU', 'CCU', 'CSICU'],\n",
    "        'severity': ['low', 'medium', 'high']\n",
    "    }\n",
    "\n",
    "config = ValidationConfig()\n",
    "print(\"Validation configuration initialized!\")\n",
    "print(f\"Results will be saved to: {config.RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dbe0f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded STFT features successfully!\n",
      "Loaded final_best_model from ensemble_models/final_best_model.pkl\n",
      "Loaded ensemble_votingsoft from ensemble_models/ensemble_votingsoft.pkl\n",
      "Loaded base_model_xgboost from ensemble_models/base_model_xgboost.pkl\n",
      "Loaded base_model_lightgbm from ensemble_models/base_model_lightgbm.pkl\n",
      "Loaded base_model_logisticregression from ensemble_models/base_model_logisticregression.pkl\n",
      "Loaded feature_selector from models/feature_selector.pkl\n",
      "Loaded xgboost_with_stft_model from models/xgboost_with_stft_model.pkl\n",
      "Loaded 7 models for validation\n",
      "Data shapes - Train: (68, 537), Test: (15, 537)\n",
      "Loaded base_model_xgboost from ensemble_models/base_model_xgboost.pkl\n",
      "Loaded base_model_lightgbm from ensemble_models/base_model_lightgbm.pkl\n",
      "Loaded base_model_logisticregression from ensemble_models/base_model_logisticregression.pkl\n",
      "Loaded feature_selector from models/feature_selector.pkl\n",
      "Loaded xgboost_with_stft_model from models/xgboost_with_stft_model.pkl\n",
      "Loaded 7 models for validation\n",
      "Data shapes - Train: (68, 537), Test: (15, 537)\n"
     ]
    }
   ],
   "source": [
    "# Load models and data for validation\n",
    "def load_validation_data():\n",
    "    \"\"\"Load models and data for comprehensive validation\"\"\"\n",
    "    \n",
    "    # Load processed data\n",
    "    try:\n",
    "        X_train = pd.read_csv(\"data/stft_features/train_stft_scaled.csv\")\n",
    "        X_test = pd.read_csv(\"data/stft_features/test_stft_scaled.csv\")\n",
    "        \n",
    "        # Load target variables from saved files\n",
    "        y_train = np.load('data/processed/y_train_stft.npy', allow_pickle=True)\n",
    "        y_test = np.load('data/processed/y_test_stft.npy', allow_pickle=True)\n",
    "        feature_type = \"STFT-enhanced\"\n",
    "        print(\"Loaded STFT features successfully!\")\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            # Extract labels from patient files if not already saved\n",
    "            train_patients = np.load('data/processed/train_patients.npy', allow_pickle=True)\n",
    "            test_patients = np.load('data/processed/test_patients.npy', allow_pickle=True)\n",
    "            \n",
    "            def load_patient_labels(patient_ids):\n",
    "                labels = []\n",
    "                for patient_id in patient_ids:\n",
    "                    try:\n",
    "                        data = pd.read_csv(f'data/raw/training_setA (1)/{patient_id}.psv', sep='|')\n",
    "                        label = data['SepsisLabel'].max()\n",
    "                        labels.append(label)\n",
    "                    except:\n",
    "                        labels.append(0)\n",
    "                return np.array(labels)\n",
    "            \n",
    "            X_train = pd.read_csv(\"data/stft_features/train_stft_scaled.csv\")\n",
    "            X_test = pd.read_csv(\"data/stft_features/test_stft_scaled.csv\")\n",
    "            y_train = load_patient_labels(train_patients)\n",
    "            y_test = load_patient_labels(test_patients)\n",
    "            feature_type = \"STFT-enhanced\"\n",
    "            print(\"Loaded STFT features and extracted labels!\")\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(\"No preprocessed data found!\")\n",
    "    \n",
    "    # Load trained models\n",
    "    models = {}\n",
    "    \n",
    "    # Check for ensemble models first\n",
    "    ensemble_paths = [\n",
    "        \"ensemble_models/final_best_model.pkl\",\n",
    "        \"ensemble_models/ensemble_votingsoft.pkl\", \n",
    "        \"ensemble_models/base_model_xgboost.pkl\",\n",
    "        \"ensemble_models/base_model_lightgbm.pkl\",\n",
    "        \"ensemble_models/base_model_logisticregression.pkl\"\n",
    "    ]\n",
    "    \n",
    "    for model_path in ensemble_paths:\n",
    "        if os.path.exists(model_path):\n",
    "            try:\n",
    "                model_name = os.path.basename(model_path).replace('.pkl', '')\n",
    "                model = joblib.load(model_path)\n",
    "                models[model_name] = model\n",
    "                print(f\"Loaded {model_name} from {model_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {model_path}: {str(e)}\")\n",
    "    \n",
    "    # Check for other trained models\n",
    "    model_paths = [\n",
    "        (\"models/baseline/\", \"_baseline.pkl\"),\n",
    "        (\"models/advanced/\", \"_optimized.pkl\"),\n",
    "        (\"models/\", \".pkl\")\n",
    "    ]\n",
    "    \n",
    "    for model_dir, suffix in model_paths:\n",
    "        if os.path.exists(model_dir):\n",
    "            for filename in os.listdir(model_dir):\n",
    "                if filename.endswith(suffix) and filename not in [f\"{m}.pkl\" for m in models.keys()]:\n",
    "                    model_name = filename.replace(suffix, \"\").replace(\".pkl\", \"\")\n",
    "                    try:\n",
    "                        model_path = os.path.join(model_dir, filename)\n",
    "                        model = joblib.load(model_path)\n",
    "                        models[model_name] = model\n",
    "                        print(f\"Loaded {model_name} from {model_path}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading {filename}: {str(e)}\")\n",
    "    \n",
    "    if not models:\n",
    "        print(\"No trained models found. Please run training notebooks first.\")\n",
    "        return None, None, None, None, None, feature_type\n",
    "    \n",
    "    print(f\"Loaded {len(models)} models for validation\")\n",
    "    print(f\"Data shapes - Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, models, feature_type\n",
    "\n",
    "# Load data and models\n",
    "X_train, X_test, y_train, y_test, models, feature_type = load_validation_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e6b68f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing temporal validation...\n",
      "Temporal validation for final_best_model...\n",
      "Error in fold 0 for final_best_model: could not convert string to float: 'p003025'\n",
      "Error in fold 1 for final_best_model: could not convert string to float: 'p003025'\n",
      "Error in fold 2 for final_best_model: could not convert string to float: 'p003025'\n",
      "Error in fold 3 for final_best_model: could not convert string to float: 'p003025'\n",
      "Error in fold 4 for final_best_model: could not convert string to float: 'p003025'\n",
      "Temporal validation for ensemble_votingsoft...\n",
      "Error in fold 0 for ensemble_votingsoft: VotingClassifier.__init__() got an unexpected keyword argument 'LogisticRegression'\n",
      "Error in fold 1 for ensemble_votingsoft: VotingClassifier.__init__() got an unexpected keyword argument 'LogisticRegression'\n",
      "Error in fold 2 for ensemble_votingsoft: VotingClassifier.__init__() got an unexpected keyword argument 'LogisticRegression'\n",
      "Error in fold 3 for ensemble_votingsoft: VotingClassifier.__init__() got an unexpected keyword argument 'LogisticRegression'\n",
      "Error in fold 4 for ensemble_votingsoft: VotingClassifier.__init__() got an unexpected keyword argument 'LogisticRegression'\n",
      "Temporal validation for base_model_xgboost...\n",
      "Error in fold 0 for base_model_xgboost: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:patient_id: object\n",
      "Error in fold 1 for base_model_xgboost: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:patient_id: object\n",
      "Error in fold 2 for base_model_xgboost: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:patient_id: object\n",
      "Error in fold 3 for base_model_xgboost: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:patient_id: object\n",
      "Error in fold 4 for base_model_xgboost: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:patient_id: object\n",
      "Temporal validation for base_model_lightgbm...\n",
      "Error in fold 0 for base_model_lightgbm: pandas dtypes must be int, float or bool.\n",
      "Fields with bad pandas dtypes: patient_id: object\n",
      "Error in fold 1 for base_model_lightgbm: pandas dtypes must be int, float or bool.\n",
      "Fields with bad pandas dtypes: patient_id: object\n",
      "Error in fold 2 for base_model_lightgbm: pandas dtypes must be int, float or bool.\n",
      "Fields with bad pandas dtypes: patient_id: object\n",
      "Error in fold 3 for base_model_lightgbm: pandas dtypes must be int, float or bool.\n",
      "Fields with bad pandas dtypes: patient_id: object\n",
      "Error in fold 4 for base_model_lightgbm: pandas dtypes must be int, float or bool.\n",
      "Fields with bad pandas dtypes: patient_id: object\n",
      "Temporal validation for base_model_logisticregression...\n",
      "Error in fold 0 for base_model_logisticregression: could not convert string to float: 'p003025'\n",
      "Error in fold 1 for base_model_logisticregression: could not convert string to float: 'p003025'\n",
      "Error in fold 2 for base_model_logisticregression: could not convert string to float: 'p003025'\n",
      "Error in fold 3 for base_model_logisticregression: could not convert string to float: 'p003025'\n",
      "Error in fold 4 for base_model_logisticregression: could not convert string to float: 'p003025'\n",
      "Temporal validation for feature_selector...\n",
      "Temporal validation for xgboost_with_stft_model...\n",
      "Error in fold 0 for xgboost_with_stft_model: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:patient_id: object\n",
      "Error in fold 1 for xgboost_with_stft_model: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:patient_id: object\n",
      "Error in fold 2 for xgboost_with_stft_model: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:patient_id: object\n",
      "Error in fold 3 for xgboost_with_stft_model: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:patient_id: object\n",
      "Error in fold 4 for xgboost_with_stft_model: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:patient_id: object\n",
      "Temporal validation plot saved to results/validation/temporal_validation_plot.html\n",
      "Temporal validation plot saved to results/validation/temporal_validation_plot.html\n"
     ]
    }
   ],
   "source": [
    "# Temporal validation implementation\n",
    "def temporal_validation(models, X_train, y_train, time_splits=5):\n",
    "    \"\"\"\n",
    "    Perform temporal validation using time-based splits\n",
    "    \"\"\"\n",
    "    print(\"Performing temporal validation...\")\n",
    "    \n",
    "    # Create time-based splits\n",
    "    tscv = TimeSeriesSplit(n_splits=time_splits)\n",
    "    temporal_results = defaultdict(list)\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Temporal validation for {model_name}...\")\n",
    "        \n",
    "        fold_results = []\n",
    "        for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train)):\n",
    "            try:\n",
    "                # Handle both DataFrame and numpy array indexing\n",
    "                if isinstance(X_train, pd.DataFrame):\n",
    "                    X_train_fold = X_train.iloc[train_idx]\n",
    "                    X_val_fold = X_train.iloc[val_idx]\n",
    "                else:\n",
    "                    X_train_fold = X_train[train_idx]\n",
    "                    X_val_fold = X_train[val_idx]\n",
    "                \n",
    "                if isinstance(y_train, pd.Series):\n",
    "                    y_train_fold = y_train.iloc[train_idx]\n",
    "                    y_val_fold = y_train.iloc[val_idx]\n",
    "                else:\n",
    "                    y_train_fold = y_train[train_idx]\n",
    "                    y_val_fold = y_train[val_idx]\n",
    "                \n",
    "                # Skip training if it's feature_selector or other non-predictive models\n",
    "                if model_name == 'feature_selector' or not hasattr(model, 'predict'):\n",
    "                    continue\n",
    "                \n",
    "                # Train model on training fold\n",
    "                model_copy = model.__class__(**model.get_params()) if hasattr(model, 'get_params') else model\n",
    "                model_copy.fit(X_train_fold, y_train_fold)\n",
    "                \n",
    "                # Evaluate on validation fold\n",
    "                y_pred = model_copy.predict(X_val_fold)\n",
    "                y_pred_proba = model_copy.predict_proba(X_val_fold)[:, 1] if hasattr(model_copy, 'predict_proba') else None\n",
    "                \n",
    "                # Calculate metrics\n",
    "                metrics = {\n",
    "                    'fold': fold,\n",
    "                    'roc_auc': roc_auc_score(y_val_fold, y_pred_proba) if y_pred_proba is not None and len(np.unique(y_val_fold)) > 1 else None,\n",
    "                    'f1_score': f1_score(y_val_fold, y_pred, zero_division=0),\n",
    "                    'sensitivity': recall_score(y_val_fold, y_pred, zero_division=0),\n",
    "                    'precision': precision_score(y_val_fold, y_pred, zero_division=0),\n",
    "                    'accuracy': accuracy_score(y_val_fold, y_pred),\n",
    "                    'train_size': len(train_idx),\n",
    "                    'val_size': len(val_idx)\n",
    "                }\n",
    "                \n",
    "                fold_results.append(metrics)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in fold {fold} for {model_name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        temporal_results[model_name] = fold_results\n",
    "    \n",
    "    return temporal_results\n",
    "\n",
    "def plot_temporal_validation(temporal_results):\n",
    "    \"\"\"Plot temporal validation results\"\"\"\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('ROC-AUC Over Time', 'F1-Score Over Time', \n",
    "                       'Sensitivity Over Time', 'Precision Over Time')\n",
    "    )\n",
    "    \n",
    "    colors = px.colors.qualitative.Set1\n",
    "    \n",
    "    for i, (model_name, results) in enumerate(temporal_results.items()):\n",
    "        if not results:\n",
    "            continue\n",
    "            \n",
    "        df = pd.DataFrame(results)\n",
    "        color = colors[i % len(colors)]\n",
    "        \n",
    "        # ROC-AUC\n",
    "        if 'roc_auc' in df.columns and df['roc_auc'].notna().any():\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=df['fold'], y=df['roc_auc'],\n",
    "                mode='lines+markers', name=f'{model_name}_AUC',\n",
    "                line=dict(color=color), showlegend=True\n",
    "            ), row=1, col=1)\n",
    "        \n",
    "        # F1-Score\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df['fold'], y=df['f1_score'],\n",
    "            mode='lines+markers', name=f'{model_name}_F1',\n",
    "            line=dict(color=color, dash='dash'), showlegend=False\n",
    "        ), row=1, col=2)\n",
    "        \n",
    "        # Sensitivity\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df['fold'], y=df['sensitivity'],\n",
    "            mode='lines+markers', name=f'{model_name}_Sens',\n",
    "            line=dict(color=color, dash='dot'), showlegend=False\n",
    "        ), row=2, col=1)\n",
    "        \n",
    "        # Precision\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df['fold'], y=df['precision'],\n",
    "            mode='lines+markers', name=f'{model_name}_Prec',\n",
    "            line=dict(color=color, dash='dashdot'), showlegend=False\n",
    "        ), row=2, col=2)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Temporal Validation Results\",\n",
    "        height=800\n",
    "    )\n",
    "    \n",
    "    # Save plot instead of showing to avoid nbformat issues\n",
    "    try:\n",
    "        fig.write_html(\"results/validation/temporal_validation_plot.html\")\n",
    "        print(\"Temporal validation plot saved to results/validation/temporal_validation_plot.html\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not save plot: {e}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Perform temporal validation\n",
    "if models:\n",
    "    temporal_results = temporal_validation(models, X_train, y_train, config.TIME_SPLITS)\n",
    "    temporal_plot = plot_temporal_validation(temporal_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7596572f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing comprehensive clinical evaluation...\n",
      "Clinical evaluation for final_best_model...\n",
      "Error evaluating final_best_model: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- Age\n",
      "- BaseExcess_stft_fast_max_power\n",
      "- BaseExcess_stft_fast_mean_power\n",
      "- BaseExcess_stft_fast_power_ratio\n",
      "- BaseExcess_stft_fast_spectral_centroid\n",
      "- ...\n",
      "\n",
      "Clinical evaluation for ensemble_votingsoft...\n",
      "Error evaluating ensemble_votingsoft: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- Age\n",
      "- BaseExcess_stft_fast_max_power\n",
      "- BaseExcess_stft_fast_mean_power\n",
      "- BaseExcess_stft_fast_power_ratio\n",
      "- BaseExcess_stft_fast_spectral_centroid\n",
      "- ...\n",
      "\n",
      "Clinical evaluation for base_model_xgboost...\n",
      "Error evaluating base_model_xgboost: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:patient_id: object\n",
      "Clinical evaluation for base_model_lightgbm...\n",
      "Error evaluating base_model_lightgbm: pandas dtypes must be int, float or bool.\n",
      "Fields with bad pandas dtypes: patient_id: object\n",
      "Clinical evaluation for base_model_logisticregression...\n",
      "Error evaluating base_model_logisticregression: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- Age\n",
      "- BaseExcess_stft_fast_max_power\n",
      "- BaseExcess_stft_fast_mean_power\n",
      "- BaseExcess_stft_fast_power_ratio\n",
      "- BaseExcess_stft_fast_spectral_centroid\n",
      "- ...\n",
      "\n",
      "Clinical evaluation for feature_selector...\n",
      "Error evaluating feature_selector: 'SelectKBest' object has no attribute 'predict'\n",
      "Clinical evaluation for xgboost_with_stft_model...\n",
      "Error evaluating xgboost_with_stft_model: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:patient_id: object\n",
      "\n",
      "=== CLINICAL PERFORMANCE METRICS ===\n"
     ]
    }
   ],
   "source": [
    "# Clinical performance metrics\n",
    "def calculate_clinical_metrics(y_true, y_pred, y_pred_proba=None):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive clinical performance metrics\n",
    "    \"\"\"\n",
    "    # Basic confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # Calculate clinical metrics\n",
    "    metrics = {\n",
    "        'True_Positives': tp,\n",
    "        'True_Negatives': tn,\n",
    "        'False_Positives': fp,\n",
    "        'False_Negatives': fn,\n",
    "        'Sensitivity': tp / (tp + fn) if (tp + fn) > 0 else 0,\n",
    "        'Specificity': tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        'PPV': tp / (tp + fp) if (tp + fp) > 0 else 0,  # Positive Predictive Value\n",
    "        'NPV': tn / (tn + fn) if (tn + fn) > 0 else 0,  # Negative Predictive Value\n",
    "        'Accuracy': (tp + tn) / (tp + tn + fp + fn),\n",
    "        'F1_Score': f1_score(y_true, y_pred),\n",
    "        'Matthews_CC': matthews_corrcoef(y_true, y_pred),\n",
    "        'Cohens_Kappa': cohen_kappa_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_true, y_pred)\n",
    "    }\n",
    "    \n",
    "    if y_pred_proba is not None:\n",
    "        metrics.update({\n",
    "            'ROC_AUC': roc_auc_score(y_true, y_pred_proba),\n",
    "            'PR_AUC': average_precision_score(y_true, y_pred_proba),\n",
    "            'Brier_Score': brier_score_loss(y_true, y_pred_proba)\n",
    "        })\n",
    "    \n",
    "    # Clinical utility scores\n",
    "    metrics['Clinical_Utility'] = (\n",
    "        0.4 * metrics['Sensitivity'] +  # High weight on catching sepsis\n",
    "        0.3 * metrics['Specificity'] +  # Moderate weight on avoiding false alarms\n",
    "        0.2 * metrics['PPV'] +          # Importance of positive predictions being correct\n",
    "        0.1 * metrics['NPV']            # Lower weight on negative predictions\n",
    "    )\n",
    "    \n",
    "    # Early detection score (assuming this model supports it)\n",
    "    metrics['Early_Detection_Score'] = metrics['Sensitivity'] * 0.8 + metrics['PPV'] * 0.2\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def comprehensive_clinical_evaluation(models, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Perform comprehensive clinical evaluation of all models\n",
    "    \"\"\"\n",
    "    print(\"Performing comprehensive clinical evaluation...\")\n",
    "    \n",
    "    clinical_results = []\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Clinical evaluation for {model_name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Make predictions\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "            \n",
    "            # Calculate clinical metrics\n",
    "            metrics = calculate_clinical_metrics(y_test, y_pred, y_pred_proba)\n",
    "            metrics['Model'] = model_name\n",
    "            \n",
    "            clinical_results.append(metrics)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {model_name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(clinical_results)\n",
    "\n",
    "# Perform clinical evaluation\n",
    "if models:\n",
    "    clinical_evaluation = comprehensive_clinical_evaluation(models, X_test, y_test)\n",
    "    \n",
    "    print(\"\\n=== CLINICAL PERFORMANCE METRICS ===\")\n",
    "    if not clinical_evaluation.empty:\n",
    "        display_cols = ['Model', 'Sensitivity', 'Specificity', 'PPV', 'NPV', 'Clinical_Utility', 'Early_Detection_Score']\n",
    "        print(clinical_evaluation[display_cols].round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2a3f1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing SHAP interpretability analysis...\n",
      "SHAP analysis for final_best_model...\n",
      "Provided model function fails when applied to the provided data set.\n",
      "Error in SHAP analysis for final_best_model: could not convert string to float: 'p003025'\n",
      "SHAP analysis for ensemble_votingsoft...\n",
      "Provided model function fails when applied to the provided data set.\n",
      "Error in SHAP analysis for ensemble_votingsoft: could not convert string to float: 'p003025'\n",
      "SHAP analysis for base_model_xgboost...\n",
      "Provided model function fails when applied to the provided data set.\n",
      "Error in SHAP analysis for base_model_xgboost: Feature shape mismatch, expected: 100, got 537\n",
      "SHAP analysis for base_model_lightgbm...\n",
      "Error in SHAP analysis for base_model_lightgbm: property 'feature_names_in_' of 'LGBMClassifier' object has no setter\n",
      "SHAP analysis for base_model_logisticregression...\n",
      "Provided model function fails when applied to the provided data set.\n",
      "Error in SHAP analysis for base_model_logisticregression: could not convert string to float: 'p003025'\n",
      "SHAP analysis for feature_selector...\n",
      "SHAP analysis for xgboost_with_stft_model...\n",
      "Provided model function fails when applied to the provided data set.\n",
      "Error in SHAP analysis for xgboost_with_stft_model: Feature shape mismatch, expected: 100, got 537\n"
     ]
    }
   ],
   "source": [
    "# Model interpretability analysis using SHAP\n",
    "def shap_interpretability_analysis(models, X_train, X_test, max_samples=1000):\n",
    "    \"\"\"\n",
    "    Perform SHAP analysis for model interpretability\n",
    "    \"\"\"\n",
    "    if not SHAP_AVAILABLE:\n",
    "        print(\"SHAP not available for interpretability analysis\")\n",
    "        return {}\n",
    "    \n",
    "    print(\"Performing SHAP interpretability analysis...\")\n",
    "    \n",
    "    shap_results = {}\n",
    "    \n",
    "    # Sample data for faster computation\n",
    "    if len(X_train) > max_samples:\n",
    "        sample_idx = np.random.choice(len(X_train), max_samples, replace=False)\n",
    "        X_train_sample = X_train.iloc[sample_idx]\n",
    "    else:\n",
    "        X_train_sample = X_train\n",
    "    \n",
    "    if len(X_test) > max_samples:\n",
    "        sample_idx = np.random.choice(len(X_test), max_samples, replace=False)\n",
    "        X_test_sample = X_test.iloc[sample_idx]\n",
    "    else:\n",
    "        X_test_sample = X_test\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        try:\n",
    "            print(f\"SHAP analysis for {model_name}...\")\n",
    "            \n",
    "            # Handle different model types\n",
    "            if hasattr(model, 'named_steps'):\n",
    "                # Pipeline models\n",
    "                classifier = model.named_steps['classifier']\n",
    "                # Transform data through pipeline preprocessing\n",
    "                X_train_transformed = model[:-1].transform(X_train_sample)\n",
    "                X_test_transformed = model[:-1].transform(X_test_sample)\n",
    "            else:\n",
    "                classifier = model\n",
    "                X_train_transformed = X_train_sample\n",
    "                X_test_transformed = X_test_sample\n",
    "            \n",
    "            # Choose appropriate SHAP explainer\n",
    "            if hasattr(classifier, 'predict_proba'):\n",
    "                if 'XGB' in model_name:\n",
    "                    explainer = shap.TreeExplainer(classifier)\n",
    "                    shap_values = explainer.shap_values(X_test_transformed)\n",
    "                elif 'Random_Forest' in model_name or 'Extra_Trees' in model_name:\n",
    "                    explainer = shap.TreeExplainer(classifier)\n",
    "                    shap_values = explainer.shap_values(X_test_transformed)\n",
    "                else:\n",
    "                    # Use KernelExplainer for other models\n",
    "                    explainer = shap.KernelExplainer(classifier.predict_proba, X_train_transformed[:100])\n",
    "                    shap_values = explainer.shap_values(X_test_transformed[:100])\n",
    "                \n",
    "                # Store results\n",
    "                if isinstance(shap_values, list):\n",
    "                    shap_values = shap_values[1]  # Get positive class SHAP values\n",
    "                \n",
    "                shap_results[model_name] = {\n",
    "                    'explainer': explainer,\n",
    "                    'shap_values': shap_values,\n",
    "                    'feature_names': X_test.columns.tolist(),\n",
    "                    'X_test_sample': X_test_transformed\n",
    "                }\n",
    "                \n",
    "                print(f\"SHAP analysis completed for {model_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in SHAP analysis for {model_name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return shap_results\n",
    "\n",
    "def plot_shap_summary(shap_results, top_n=20):\n",
    "    \"\"\"\n",
    "    Create SHAP summary plots\n",
    "    \"\"\"\n",
    "    if not shap_results:\n",
    "        print(\"No SHAP results to plot\")\n",
    "        return None\n",
    "    \n",
    "    # Feature importance aggregation across models\n",
    "    feature_importance_agg = defaultdict(list)\n",
    "    \n",
    "    for model_name, results in shap_results.items():\n",
    "        shap_values = results['shap_values']\n",
    "        feature_names = results['feature_names']\n",
    "        \n",
    "        # Calculate mean absolute SHAP values\n",
    "        mean_shap = np.abs(shap_values).mean(axis=0)\n",
    "        \n",
    "        for feature, importance in zip(feature_names, mean_shap):\n",
    "            feature_importance_agg[feature].append(importance)\n",
    "    \n",
    "    # Aggregate importance across models\n",
    "    aggregated_importance = {}\n",
    "    for feature, importances in feature_importance_agg.items():\n",
    "        aggregated_importance[feature] = {\n",
    "            'mean': np.mean(importances),\n",
    "            'std': np.std(importances),\n",
    "            'count': len(importances)\n",
    "        }\n",
    "    \n",
    "    # Sort by mean importance\n",
    "    sorted_features = sorted(aggregated_importance.items(), \n",
    "                           key=lambda x: x[1]['mean'], reverse=True)\n",
    "    \n",
    "    # Create plot\n",
    "    top_features = sorted_features[:top_n]\n",
    "    feature_names = [f[0] for f in top_features]\n",
    "    mean_importance = [f[1]['mean'] for f in top_features]\n",
    "    std_importance = [f[1]['std'] for f in top_features]\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Bar(\n",
    "        x=mean_importance,\n",
    "        y=feature_names,\n",
    "        orientation='h',\n",
    "        error_x=dict(type='data', array=std_importance),\n",
    "        name='Feature Importance'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"Top {top_n} Features by SHAP Importance (Aggregated)\",\n",
    "        xaxis_title=\"Mean |SHAP Value|\",\n",
    "        yaxis_title=\"Features\",\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    return fig, aggregated_importance\n",
    "\n",
    "# Perform SHAP analysis\n",
    "if models and SHAP_AVAILABLE:\n",
    "    shap_results = shap_interpretability_analysis(models, X_train, X_test)\n",
    "    if shap_results:\n",
    "        shap_plot, feature_importance_shap = plot_shap_summary(shap_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1879dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing fairness evaluation...\n",
      "Fairness analysis for final_best_model...\n",
      "Error in fairness evaluation for final_best_model: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- Age\n",
      "- BaseExcess_stft_fast_max_power\n",
      "- BaseExcess_stft_fast_mean_power\n",
      "- BaseExcess_stft_fast_power_ratio\n",
      "- BaseExcess_stft_fast_spectral_centroid\n",
      "- ...\n",
      "\n",
      "Fairness analysis for ensemble_votingsoft...\n",
      "Error in fairness evaluation for ensemble_votingsoft: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- Age\n",
      "- BaseExcess_stft_fast_max_power\n",
      "- BaseExcess_stft_fast_mean_power\n",
      "- BaseExcess_stft_fast_power_ratio\n",
      "- BaseExcess_stft_fast_spectral_centroid\n",
      "- ...\n",
      "\n",
      "Fairness analysis for base_model_xgboost...\n",
      "Error in fairness evaluation for base_model_xgboost: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:patient_id: object\n",
      "Fairness analysis for base_model_lightgbm...\n",
      "Error in fairness evaluation for base_model_lightgbm: pandas dtypes must be int, float or bool.\n",
      "Fields with bad pandas dtypes: patient_id: object\n",
      "Fairness analysis for base_model_logisticregression...\n",
      "Error in fairness evaluation for base_model_logisticregression: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- Age\n",
      "- BaseExcess_stft_fast_max_power\n",
      "- BaseExcess_stft_fast_mean_power\n",
      "- BaseExcess_stft_fast_power_ratio\n",
      "- BaseExcess_stft_fast_spectral_centroid\n",
      "- ...\n",
      "\n",
      "Fairness analysis for feature_selector...\n",
      "Error in fairness evaluation for feature_selector: 'SelectKBest' object has no attribute 'predict'\n",
      "Fairness analysis for xgboost_with_stft_model...\n",
      "Error in fairness evaluation for xgboost_with_stft_model: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:patient_id: object\n"
     ]
    }
   ],
   "source": [
    "# Bias and fairness evaluation\n",
    "def create_synthetic_demographics(n_samples):\n",
    "    \"\"\"\n",
    "    Create synthetic demographic data for bias analysis\n",
    "    Note: In real applications, use actual demographic data\n",
    "    \"\"\"\n",
    "    np.random.seed(config.RANDOM_STATE)\n",
    "    \n",
    "    demographics = pd.DataFrame({\n",
    "        'age': np.random.choice(['18-65', '65-80', '80+'], n_samples, p=[0.6, 0.3, 0.1]),\n",
    "        'gender': np.random.choice(['M', 'F'], n_samples, p=[0.55, 0.45]),\n",
    "        'ethnicity': np.random.choice(['White', 'Black', 'Hispanic', 'Asian', 'Other'], \n",
    "                                    n_samples, p=[0.6, 0.15, 0.15, 0.07, 0.03]),\n",
    "        'insurance': np.random.choice(['Private', 'Medicare', 'Medicaid', 'Uninsured'], \n",
    "                                    n_samples, p=[0.5, 0.25, 0.2, 0.05])\n",
    "    })\n",
    "    \n",
    "    return demographics\n",
    "\n",
    "def fairness_evaluation(models, X_test, y_test, demographics):\n",
    "    \"\"\"\n",
    "    Evaluate model fairness across different demographic groups\n",
    "    \"\"\"\n",
    "    print(\"Performing fairness evaluation...\")\n",
    "    \n",
    "    fairness_results = []\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Fairness analysis for {model_name}...\")\n",
    "        \n",
    "        try:\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "            \n",
    "            # Analyze by each demographic attribute\n",
    "            for attribute in demographics.columns:\n",
    "                for group in demographics[attribute].unique():\n",
    "                    mask = demographics[attribute] == group\n",
    "                    \n",
    "                    if mask.sum() < 10:  # Skip small groups\n",
    "                        continue\n",
    "                    \n",
    "                    y_true_group = y_test[mask]\n",
    "                    y_pred_group = y_pred[mask]\n",
    "                    y_pred_proba_group = y_pred_proba[mask] if y_pred_proba is not None else None\n",
    "                    \n",
    "                    # Calculate metrics for this group\n",
    "                    group_metrics = calculate_clinical_metrics(y_true_group, y_pred_group, y_pred_proba_group)\n",
    "                    group_metrics.update({\n",
    "                        'Model': model_name,\n",
    "                        'Attribute': attribute,\n",
    "                        'Group': group,\n",
    "                        'Sample_Size': mask.sum(),\n",
    "                        'Positive_Rate': y_true_group.mean()\n",
    "                    })\n",
    "                    \n",
    "                    fairness_results.append(group_metrics)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in fairness evaluation for {model_name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(fairness_results)\n",
    "\n",
    "def plot_fairness_analysis(fairness_df):\n",
    "    \"\"\"\n",
    "    Plot fairness analysis results\n",
    "    \"\"\"\n",
    "    if fairness_df.empty:\n",
    "        print(\"No fairness results to plot\")\n",
    "        return None\n",
    "    \n",
    "    # Create subplots for different metrics\n",
    "    metrics_to_plot = ['Sensitivity', 'Specificity', 'PPV', 'NPV']\n",
    "    attributes = fairness_df['Attribute'].unique()\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=len(attributes), cols=len(metrics_to_plot),\n",
    "        subplot_titles=[f\"{attr} - {metric}\" for attr in attributes for metric in metrics_to_plot],\n",
    "        vertical_spacing=0.1\n",
    "    )\n",
    "    \n",
    "    colors = px.colors.qualitative.Set1\n",
    "    \n",
    "    for i, attribute in enumerate(attributes):\n",
    "        attr_data = fairness_df[fairness_df['Attribute'] == attribute]\n",
    "        \n",
    "        for j, metric in enumerate(metrics_to_plot):\n",
    "            for k, model in enumerate(attr_data['Model'].unique()):\n",
    "                model_data = attr_data[attr_data['Model'] == model]\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Bar(\n",
    "                        x=model_data['Group'],\n",
    "                        y=model_data[metric],\n",
    "                        name=f\"{model}_{metric}\" if i == 0 and j == 0 else \"\",\n",
    "                        marker_color=colors[k % len(colors)],\n",
    "                        showlegend=True if i == 0 and j == 0 else False\n",
    "                    ),\n",
    "                    row=i+1, col=j+1\n",
    "                )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Fairness Analysis Across Demographic Groups\",\n",
    "        height=300 * len(attributes)\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    return fig\n",
    "\n",
    "# Create synthetic demographics and perform fairness evaluation\n",
    "if models:\n",
    "    demographics = create_synthetic_demographics(len(X_test))\n",
    "    fairness_results = fairness_evaluation(models, X_test, y_test, demographics)\n",
    "    \n",
    "    if not fairness_results.empty:\n",
    "        fairness_plot = plot_fairness_analysis(fairness_results)\n",
    "        \n",
    "        print(\"\\n=== FAIRNESS EVALUATION SUMMARY ===\")\n",
    "        # Calculate fairness metrics\n",
    "        for attribute in demographics.columns:\n",
    "            attr_results = fairness_results[fairness_results['Attribute'] == attribute]\n",
    "            if not attr_results.empty:\n",
    "                print(f\"\\n{attribute.upper()}:\")\n",
    "                sens_by_group = attr_results.groupby('Group')['Sensitivity'].mean()\n",
    "                spec_by_group = attr_results.groupby('Group')['Specificity'].mean()\n",
    "                print(f\"  Sensitivity range: {sens_by_group.min():.3f} - {sens_by_group.max():.3f}\")\n",
    "                print(f\"  Specificity range: {spec_by_group.min():.3f} - {spec_by_group.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d66b29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing robustness testing...\n",
      "Robustness trial 1/5\n",
      "Error in trial 0 for final_best_model: can only concatenate str (not \"float\") to str\n",
      "Error in trial 0 for ensemble_votingsoft: can only concatenate str (not \"float\") to str\n",
      "Error in trial 0 for base_model_xgboost: can only concatenate str (not \"float\") to str\n",
      "Error in trial 0 for base_model_lightgbm: can only concatenate str (not \"float\") to str\n",
      "Error in trial 0 for base_model_logisticregression: can only concatenate str (not \"float\") to str\n",
      "Error in trial 0 for feature_selector: can only concatenate str (not \"float\") to str\n",
      "Error in trial 0 for xgboost_with_stft_model: can only concatenate str (not \"float\") to str\n",
      "Robustness trial 2/5\n",
      "Error in trial 1 for final_best_model: can only concatenate str (not \"float\") to str\n",
      "Error in trial 1 for ensemble_votingsoft: can only concatenate str (not \"float\") to str\n",
      "Error in trial 1 for base_model_xgboost: can only concatenate str (not \"float\") to str\n",
      "Error in trial 1 for base_model_lightgbm: can only concatenate str (not \"float\") to str\n",
      "Error in trial 1 for base_model_logisticregression: can only concatenate str (not \"float\") to str\n",
      "Error in trial 1 for feature_selector: can only concatenate str (not \"float\") to str\n",
      "Error in trial 1 for xgboost_with_stft_model: can only concatenate str (not \"float\") to str\n",
      "Robustness trial 3/5\n",
      "Error in trial 2 for final_best_model: can only concatenate str (not \"float\") to str\n",
      "Error in trial 2 for ensemble_votingsoft: can only concatenate str (not \"float\") to str\n",
      "Error in trial 2 for base_model_xgboost: can only concatenate str (not \"float\") to str\n",
      "Error in trial 2 for base_model_lightgbm: can only concatenate str (not \"float\") to str\n",
      "Error in trial 2 for base_model_logisticregression: can only concatenate str (not \"float\") to str\n",
      "Error in trial 2 for feature_selector: can only concatenate str (not \"float\") to str\n",
      "Error in trial 2 for xgboost_with_stft_model: can only concatenate str (not \"float\") to str\n",
      "Robustness trial 4/5\n",
      "Error in trial 3 for final_best_model: can only concatenate str (not \"float\") to str\n",
      "Error in trial 3 for ensemble_votingsoft: can only concatenate str (not \"float\") to str\n",
      "Error in trial 3 for base_model_xgboost: can only concatenate str (not \"float\") to str\n",
      "Error in trial 3 for base_model_lightgbm: can only concatenate str (not \"float\") to str\n",
      "Error in trial 3 for base_model_logisticregression: can only concatenate str (not \"float\") to str\n",
      "Error in trial 3 for feature_selector: can only concatenate str (not \"float\") to str\n",
      "Error in trial 3 for xgboost_with_stft_model: can only concatenate str (not \"float\") to str\n",
      "Robustness trial 5/5\n",
      "Error in trial 4 for final_best_model: can only concatenate str (not \"float\") to str\n",
      "Error in trial 4 for ensemble_votingsoft: can only concatenate str (not \"float\") to str\n",
      "Error in trial 4 for base_model_xgboost: can only concatenate str (not \"float\") to str\n",
      "Error in trial 4 for base_model_lightgbm: can only concatenate str (not \"float\") to str\n",
      "Error in trial 4 for base_model_logisticregression: can only concatenate str (not \"float\") to str\n",
      "Error in trial 4 for feature_selector: can only concatenate str (not \"float\") to str\n",
      "Error in trial 4 for xgboost_with_stft_model: can only concatenate str (not \"float\") to str\n",
      "No robustness results to analyze\n"
     ]
    }
   ],
   "source": [
    "# Robustness and stability testing\n",
    "def robustness_testing(models, X_train, y_train, X_test, y_test, n_trials=10):\n",
    "    \"\"\"\n",
    "    Test model robustness across multiple random seeds and data perturbations\n",
    "    \"\"\"\n",
    "    print(\"Performing robustness testing...\")\n",
    "    \n",
    "    robustness_results = []\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        print(f\"Robustness trial {trial + 1}/{n_trials}\")\n",
    "        \n",
    "        # Set different random seed for each trial\n",
    "        np.random.seed(config.RANDOM_STATE + trial)\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            try:\n",
    "                # Add small amount of noise to test data (data perturbation)\n",
    "                noise_level = 0.01\n",
    "                X_test_perturbed = X_test + np.random.normal(0, noise_level, X_test.shape)\n",
    "                \n",
    "                # Retrain model with different random seed if applicable\n",
    "                if hasattr(model, 'random_state'):\n",
    "                    model_params = model.get_params()\n",
    "                    model_params['random_state'] = config.RANDOM_STATE + trial\n",
    "                    model_trial = model.__class__(**model_params)\n",
    "                else:\n",
    "                    model_trial = model\n",
    "                \n",
    "                # Train and evaluate\n",
    "                model_trial.fit(X_train, y_train)\n",
    "                y_pred = model_trial.predict(X_test_perturbed)\n",
    "                y_pred_proba = model_trial.predict_proba(X_test_perturbed)[:, 1] if hasattr(model_trial, 'predict_proba') else None\n",
    "                \n",
    "                # Calculate metrics\n",
    "                metrics = calculate_clinical_metrics(y_test, y_pred, y_pred_proba)\n",
    "                metrics.update({\n",
    "                    'Model': model_name,\n",
    "                    'Trial': trial,\n",
    "                    'Noise_Level': noise_level\n",
    "                })\n",
    "                \n",
    "                robustness_results.append(metrics)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in trial {trial} for {model_name}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    return pd.DataFrame(robustness_results)\n",
    "\n",
    "def analyze_robustness(robustness_df):\n",
    "    \"\"\"\n",
    "    Analyze robustness test results\n",
    "    \"\"\"\n",
    "    if robustness_df.empty:\n",
    "        print(\"No robustness results to analyze\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate stability metrics\n",
    "    stability_metrics = []\n",
    "    \n",
    "    for model in robustness_df['Model'].unique():\n",
    "        model_data = robustness_df[robustness_df['Model'] == model]\n",
    "        \n",
    "        stability = {\n",
    "            'Model': model,\n",
    "            'Mean_ROC_AUC': model_data['ROC_AUC'].mean() if 'ROC_AUC' in model_data.columns else None,\n",
    "            'Std_ROC_AUC': model_data['ROC_AUC'].std() if 'ROC_AUC' in model_data.columns else None,\n",
    "            'Mean_Sensitivity': model_data['Sensitivity'].mean(),\n",
    "            'Std_Sensitivity': model_data['Sensitivity'].std(),\n",
    "            'Mean_Specificity': model_data['Specificity'].mean(),\n",
    "            'Std_Specificity': model_data['Specificity'].std(),\n",
    "            'CV_Sensitivity': model_data['Sensitivity'].std() / model_data['Sensitivity'].mean(),\n",
    "            'CV_Specificity': model_data['Specificity'].std() / model_data['Specificity'].mean()\n",
    "        }\n",
    "        \n",
    "        stability_metrics.append(stability)\n",
    "    \n",
    "    stability_df = pd.DataFrame(stability_metrics)\n",
    "    \n",
    "    print(\"\\n=== ROBUSTNESS ANALYSIS ===\")\n",
    "    print(\"Lower coefficient of variation (CV) indicates higher stability\")\n",
    "    display_cols = ['Model', 'Mean_Sensitivity', 'CV_Sensitivity', 'Mean_Specificity', 'CV_Specificity']\n",
    "    if not stability_df.empty:\n",
    "        print(stability_df[display_cols].round(4).to_string(index=False))\n",
    "    \n",
    "    return stability_df\n",
    "\n",
    "# Perform robustness testing\n",
    "if models:\n",
    "    robustness_results = robustness_testing(models, X_train, y_train, X_test, y_test, n_trials=5)\n",
    "    stability_analysis = analyze_robustness(robustness_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6de028b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing calibration analysis...\n",
      "Error in calibration analysis for final_best_model: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- Age\n",
      "- BaseExcess_stft_fast_max_power\n",
      "- BaseExcess_stft_fast_mean_power\n",
      "- BaseExcess_stft_fast_power_ratio\n",
      "- BaseExcess_stft_fast_spectral_centroid\n",
      "- ...\n",
      "\n",
      "Error in calibration analysis for ensemble_votingsoft: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- Age\n",
      "- BaseExcess_stft_fast_max_power\n",
      "- BaseExcess_stft_fast_mean_power\n",
      "- BaseExcess_stft_fast_power_ratio\n",
      "- BaseExcess_stft_fast_spectral_centroid\n",
      "- ...\n",
      "\n",
      "Error in calibration analysis for base_model_xgboost: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:patient_id: object\n",
      "Error in calibration analysis for base_model_lightgbm: pandas dtypes must be int, float or bool.\n",
      "Fields with bad pandas dtypes: patient_id: object\n",
      "Error in calibration analysis for base_model_logisticregression: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- Age\n",
      "- BaseExcess_stft_fast_max_power\n",
      "- BaseExcess_stft_fast_mean_power\n",
      "- BaseExcess_stft_fast_power_ratio\n",
      "- BaseExcess_stft_fast_spectral_centroid\n",
      "- ...\n",
      "\n",
      "Error in calibration analysis for xgboost_with_stft_model: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:patient_id: object\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 129\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# Perform calibration analysis\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m models:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     calibration_results, calibration_plot = \u001b[43mcalibration_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== CALIBRATION ANALYSIS ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    132\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBrier Score (lower is better):\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 123\u001b[39m, in \u001b[36mcalibration_analysis\u001b[39m\u001b[34m(models, X_test, y_test)\u001b[39m\n\u001b[32m    120\u001b[39m fig.update_xaxes(title_text=\u001b[33m\"\u001b[39m\u001b[33mFalse Positive Rate\u001b[39m\u001b[33m\"\u001b[39m, row=\u001b[32m2\u001b[39m, col=\u001b[32m2\u001b[39m)\n\u001b[32m    121\u001b[39m fig.update_yaxes(title_text=\u001b[33m\"\u001b[39m\u001b[33mTrue Positive Rate\u001b[39m\u001b[33m\"\u001b[39m, row=\u001b[32m2\u001b[39m, col=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[43mfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m calibration_results, fig\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\plotly\\basedatatypes.py:3420\u001b[39m, in \u001b[36mBaseFigure.show\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3387\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3388\u001b[39m \u001b[33;03mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[32m   3389\u001b[39m \u001b[33;03mspecified by the renderer argument\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3416\u001b[39m \u001b[33;03mNone\u001b[39;00m\n\u001b[32m   3417\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3418\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpio\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3420\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\plotly\\io\\_renderers.py:415\u001b[39m, in \u001b[36mshow\u001b[39m\u001b[34m(fig, renderer, validate, **kwargs)\u001b[39m\n\u001b[32m    410\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    411\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    412\u001b[39m     )\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nbformat \u001b[38;5;129;01mor\u001b[39;00m Version(nbformat.__version__) < Version(\u001b[33m\"\u001b[39m\u001b[33m4.2.0\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    416\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    417\u001b[39m     )\n\u001b[32m    419\u001b[39m display_jupyter_version_warnings()\n\u001b[32m    421\u001b[39m ipython_display.display(bundle, raw=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mValueError\u001b[39m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    }
   ],
   "source": [
    "# Model calibration analysis\n",
    "def calibration_analysis(models, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Analyze model calibration using reliability diagrams\n",
    "    \"\"\"\n",
    "    print(\"Performing calibration analysis...\")\n",
    "    \n",
    "    calibration_results = {}\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=['Calibration Plot', 'Calibration Scores', 'Prediction Histograms', 'ROC Curves']\n",
    "    )\n",
    "    \n",
    "    colors = px.colors.qualitative.Set1\n",
    "    \n",
    "    for i, (model_name, model) in enumerate(models.items()):\n",
    "        try:\n",
    "            if not hasattr(model, 'predict_proba'):\n",
    "                continue\n",
    "            \n",
    "            y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # Calibration curve\n",
    "            fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "                y_test, y_pred_proba, n_bins=10\n",
    "            )\n",
    "            \n",
    "            # Brier score (lower is better)\n",
    "            brier_score = brier_score_loss(y_test, y_pred_proba)\n",
    "            \n",
    "            # Store results\n",
    "            calibration_results[model_name] = {\n",
    "                'brier_score': brier_score,\n",
    "                'calibration_curve': (fraction_of_positives, mean_predicted_value),\n",
    "                'predictions': y_pred_proba\n",
    "            }\n",
    "            \n",
    "            color = colors[i % len(colors)]\n",
    "            \n",
    "            # Plot calibration curve\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=mean_predicted_value,\n",
    "                    y=fraction_of_positives,\n",
    "                    mode='lines+markers',\n",
    "                    name=f'{model_name} (Brier: {brier_score:.3f})',\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "            \n",
    "            # Plot prediction histogram\n",
    "            fig.add_trace(\n",
    "                go.Histogram(\n",
    "                    x=y_pred_proba,\n",
    "                    name=f'{model_name} predictions',\n",
    "                    opacity=0.7,\n",
    "                    nbinsx=20,\n",
    "                    marker_color=color\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "            \n",
    "            # ROC curve\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "            auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=fpr,\n",
    "                    y=tpr,\n",
    "                    mode='lines',\n",
    "                    name=f'{model_name} (AUC: {auc_score:.3f})',\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=2, col=2\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in calibration analysis for {model_name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Add perfect calibration line\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[0, 1],\n",
    "            y=[0, 1],\n",
    "            mode='lines',\n",
    "            name='Perfect Calibration',\n",
    "            line=dict(color='gray', dash='dash'),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add random classifier line for ROC\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[0, 1],\n",
    "            y=[0, 1],\n",
    "            mode='lines',\n",
    "            name='Random Classifier',\n",
    "            line=dict(color='gray', dash='dash'),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Model Calibration Analysis\",\n",
    "        height=800\n",
    "    )\n",
    "    \n",
    "    # Update axes labels\n",
    "    fig.update_xaxes(title_text=\"Mean Predicted Probability\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Fraction of Positives\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Predicted Probability\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Count\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"False Positive Rate\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"True Positive Rate\", row=2, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return calibration_results, fig\n",
    "\n",
    "# Perform calibration analysis\n",
    "if models:\n",
    "    calibration_results, calibration_plot = calibration_analysis(models, X_test, y_test)\n",
    "    \n",
    "    print(\"\\n=== CALIBRATION ANALYSIS ===\")\n",
    "    print(\"Brier Score (lower is better):\")\n",
    "    for model_name, results in calibration_results.items():\n",
    "        print(f\"  {model_name}: {results['brier_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e51a172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive validation report generation\n",
    "def generate_comprehensive_validation_report(\n",
    "    temporal_results, clinical_evaluation, fairness_results, \n",
    "    robustness_results, stability_analysis, calibration_results,\n",
    "    shap_results=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate comprehensive validation report\n",
    "    \"\"\"\n",
    "    print(\"Generating comprehensive validation report...\")\n",
    "    \n",
    "    report_sections = []\n",
    "    \n",
    "    # Header\n",
    "    report_sections.append(\"SEPSIS PREDICTION MODEL - COMPREHENSIVE VALIDATION REPORT\")\n",
    "    report_sections.append(\"=\" * 80)\n",
    "    report_sections.append(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    report_sections.append(f\"Feature type: {feature_type}\")\n",
    "    report_sections.append(f\"Number of models evaluated: {len(models)}\")\n",
    "    report_sections.append(\"\")\n",
    "    \n",
    "    # Executive Summary\n",
    "    report_sections.append(\"EXECUTIVE SUMMARY\")\n",
    "    report_sections.append(\"-\" * 40)\n",
    "    \n",
    "    if not clinical_evaluation.empty:\n",
    "        best_model = clinical_evaluation.loc[clinical_evaluation['Clinical_Utility'].idxmax()]\n",
    "        report_sections.append(f\"Best overall model: {best_model['Model']}\")\n",
    "        report_sections.append(f\"Clinical utility score: {best_model['Clinical_Utility']:.4f}\")\n",
    "        report_sections.append(f\"Sensitivity: {best_model['Sensitivity']:.4f}\")\n",
    "        report_sections.append(f\"Specificity: {best_model['Specificity']:.4f}\")\n",
    "    \n",
    "    report_sections.append(\"\")\n",
    "    \n",
    "    # Clinical Performance\n",
    "    report_sections.append(\"CLINICAL PERFORMANCE ANALYSIS\")\n",
    "    report_sections.append(\"-\" * 50)\n",
    "    \n",
    "    if not clinical_evaluation.empty:\n",
    "        report_sections.append(\"Key Clinical Metrics:\")\n",
    "        for _, row in clinical_evaluation.iterrows():\n",
    "            report_sections.append(f\"  {row['Model']}:\")\n",
    "            report_sections.append(f\"    Sensitivity: {row['Sensitivity']:.4f}\")\n",
    "            report_sections.append(f\"    Specificity: {row['Specificity']:.4f}\")\n",
    "            report_sections.append(f\"    PPV: {row['PPV']:.4f}\")\n",
    "            report_sections.append(f\"    NPV: {row['NPV']:.4f}\")\n",
    "        \n",
    "        report_sections.append(\"\")\n",
    "        \n",
    "        # Clinical recommendations\n",
    "        high_sens_models = clinical_evaluation[clinical_evaluation['Sensitivity'] >= 0.85]\n",
    "        if not high_sens_models.empty:\n",
    "            report_sections.append(\"Models meeting high sensitivity requirement (0.85):\")\n",
    "            for _, row in high_sens_models.iterrows():\n",
    "                report_sections.append(f\"  - {row['Model']} (Sensitivity: {row['Sensitivity']:.4f})\")\n",
    "        else:\n",
    "            report_sections.append(\"WARNING: No models meet high sensitivity requirement (0.85)\")\n",
    "    \n",
    "    report_sections.append(\"\")\n",
    "    \n",
    "    # Temporal Validation\n",
    "    report_sections.append(\"TEMPORAL VALIDATION ANALYSIS\")\n",
    "    report_sections.append(\"-\" * 45)\n",
    "    \n",
    "    if temporal_results:\n",
    "        report_sections.append(\"Temporal stability analysis:\")\n",
    "        for model_name, results in temporal_results.items():\n",
    "            if results:\n",
    "                df = pd.DataFrame(results)\n",
    "                if 'roc_auc' in df.columns:\n",
    "                    mean_auc = df['roc_auc'].mean()\n",
    "                    std_auc = df['roc_auc'].std()\n",
    "                    report_sections.append(f\"  {model_name}: ROC-AUC {mean_auc:.4f}  {std_auc:.4f}\")\n",
    "    \n",
    "    report_sections.append(\"\")\n",
    "    \n",
    "    # Fairness Analysis\n",
    "    report_sections.append(\"FAIRNESS AND BIAS ANALYSIS\")\n",
    "    report_sections.append(\"-\" * 40)\n",
    "    \n",
    "    if not fairness_results.empty:\n",
    "        report_sections.append(\"Performance across demographic groups:\")\n",
    "        for attribute in fairness_results['Attribute'].unique():\n",
    "            attr_data = fairness_results[fairness_results['Attribute'] == attribute]\n",
    "            sens_range = attr_data['Sensitivity'].max() - attr_data['Sensitivity'].min()\n",
    "            spec_range = attr_data['Specificity'].max() - attr_data['Specificity'].min()\n",
    "            report_sections.append(f\"  {attribute}:\")\n",
    "            report_sections.append(f\"    Sensitivity variation: {sens_range:.4f}\")\n",
    "            report_sections.append(f\"    Specificity variation: {spec_range:.4f}\")\n",
    "    \n",
    "    report_sections.append(\"\")\n",
    "    \n",
    "    # Robustness Analysis\n",
    "    report_sections.append(\"ROBUSTNESS AND STABILITY ANALYSIS\")\n",
    "    report_sections.append(\"-\" * 45)\n",
    "    \n",
    "    if stability_analysis is not None and not stability_analysis.empty:\n",
    "        report_sections.append(\"Model stability (coefficient of variation):\")\n",
    "        for _, row in stability_analysis.iterrows():\n",
    "            report_sections.append(f\"  {row['Model']}:\")\n",
    "            report_sections.append(f\"    Sensitivity CV: {row['CV_Sensitivity']:.4f}\")\n",
    "            report_sections.append(f\"    Specificity CV: {row['CV_Specificity']:.4f}\")\n",
    "    \n",
    "    report_sections.append(\"\")\n",
    "    \n",
    "    # Model Calibration\n",
    "    report_sections.append(\"MODEL CALIBRATION ANALYSIS\")\n",
    "    report_sections.append(\"-\" * 40)\n",
    "    \n",
    "    if calibration_results:\n",
    "        report_sections.append(\"Brier scores (lower is better):\")\n",
    "        for model_name, results in calibration_results.items():\n",
    "            report_sections.append(f\"  {model_name}: {results['brier_score']:.4f}\")\n",
    "    \n",
    "    report_sections.append(\"\")\n",
    "    \n",
    "    # Feature Importance (if SHAP available)\n",
    "    if shap_results:\n",
    "        report_sections.append(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "        report_sections.append(\"-\" * 45)\n",
    "        report_sections.append(\"Top 10 most important features (averaged across models):\")\n",
    "        \n",
    "        if 'feature_importance_shap' in globals():\n",
    "            sorted_features = sorted(feature_importance_shap.items(), \n",
    "                                   key=lambda x: x[1]['mean'], reverse=True)[:10]\n",
    "            for i, (feature, importance) in enumerate(sorted_features, 1):\n",
    "                report_sections.append(f\"  {i}. {feature}: {importance['mean']:.4f}\")\n",
    "    \n",
    "    report_sections.append(\"\")\n",
    "    \n",
    "    # Recommendations\n",
    "    report_sections.append(\"CLINICAL DEPLOYMENT RECOMMENDATIONS\")\n",
    "    report_sections.append(\"-\" * 50)\n",
    "    \n",
    "    if not clinical_evaluation.empty:\n",
    "        best_clinical = clinical_evaluation.loc[clinical_evaluation['Clinical_Utility'].idxmax()]\n",
    "        most_sensitive = clinical_evaluation.loc[clinical_evaluation['Sensitivity'].idxmax()]\n",
    "        \n",
    "        report_sections.append(\"Recommended models for clinical deployment:\")\n",
    "        report_sections.append(f\"1. Primary recommendation: {best_clinical['Model']}\")\n",
    "        report_sections.append(f\"   - Highest clinical utility score: {best_clinical['Clinical_Utility']:.4f}\")\n",
    "        report_sections.append(f\"   - Balanced performance across metrics\")\n",
    "        report_sections.append(\"\")\n",
    "        \n",
    "        if most_sensitive['Model'] != best_clinical['Model']:\n",
    "            report_sections.append(f\"2. High-sensitivity option: {most_sensitive['Model']}\")\n",
    "            report_sections.append(f\"   - Highest sensitivity: {most_sensitive['Sensitivity']:.4f}\")\n",
    "            report_sections.append(f\"   - Optimal for early sepsis detection\")\n",
    "    \n",
    "    report_sections.append(\"\")\n",
    "    report_sections.append(\"VALIDATION COMPLETED SUCCESSFULLY\")\n",
    "    report_sections.append(\"=\" * 80)\n",
    "    \n",
    "    # Save report\n",
    "    report_content = \"\\n\".join(report_sections)\n",
    "    report_path = f\"{config.RESULTS_PATH}comprehensive_validation_report.txt\"\n",
    "    \n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(report_content)\n",
    "    \n",
    "    print(f\"Comprehensive validation report saved to: {report_path}\")\n",
    "    \n",
    "    return report_content\n",
    "\n",
    "# Generate comprehensive validation report\n",
    "if models:\n",
    "    validation_report = generate_comprehensive_validation_report(\n",
    "        temporal_results, clinical_evaluation, fairness_results,\n",
    "        robustness_results, stability_analysis, calibration_results,\n",
    "        shap_results if 'shap_results' in globals() else None\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== VALIDATION COMPLETE ===\")\n",
    "    print(\"All validation analyses have been completed!\")\n",
    "    print(f\"Results saved to: {config.RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca4002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all validation results\n",
    "def save_all_validation_results():\n",
    "    \"\"\"\n",
    "    Save all validation results to files\n",
    "    \"\"\"\n",
    "    print(\"Saving all validation results...\")\n",
    "    \n",
    "    # Save individual result components\n",
    "    if 'temporal_results' in globals() and temporal_results:\n",
    "        temporal_df = pd.concat([\n",
    "            pd.DataFrame(results).assign(Model=model_name) \n",
    "            for model_name, results in temporal_results.items()\n",
    "        ])\n",
    "        temporal_df.to_csv(f\"{config.RESULTS_PATH}temporal_validation_results.csv\", index=False)\n",
    "    \n",
    "    if 'clinical_evaluation' in globals() and not clinical_evaluation.empty:\n",
    "        clinical_evaluation.to_csv(f\"{config.RESULTS_PATH}clinical_evaluation_results.csv\", index=False)\n",
    "    \n",
    "    if 'fairness_results' in globals() and not fairness_results.empty:\n",
    "        fairness_results.to_csv(f\"{config.RESULTS_PATH}fairness_analysis_results.csv\", index=False)\n",
    "    \n",
    "    if 'robustness_results' in globals() and not robustness_results.empty:\n",
    "        robustness_results.to_csv(f\"{config.RESULTS_PATH}robustness_testing_results.csv\", index=False)\n",
    "    \n",
    "    if 'stability_analysis' in globals() and stability_analysis is not None:\n",
    "        stability_analysis.to_csv(f\"{config.RESULTS_PATH}stability_analysis_results.csv\", index=False)\n",
    "    \n",
    "    # Save calibration results\n",
    "    if 'calibration_results' in globals() and calibration_results:\n",
    "        import json\n",
    "        calibration_summary = {\n",
    "            model_name: {'brier_score': results['brier_score']}\n",
    "            for model_name, results in calibration_results.items()\n",
    "        }\n",
    "        with open(f\"{config.RESULTS_PATH}calibration_results.json\", 'w') as f:\n",
    "            json.dump(calibration_summary, f, indent=2)\n",
    "    \n",
    "    # Save SHAP results summary\n",
    "    if 'shap_results' in globals() and shap_results:\n",
    "        shap_summary = {\n",
    "            model_name: f\"SHAP analysis completed with {len(results['shap_values'])} samples\"\n",
    "            for model_name, results in shap_results.items()\n",
    "        }\n",
    "        with open(f\"{config.RESULTS_PATH}shap_analysis_summary.json\", 'w') as f:\n",
    "            json.dump(shap_summary, f, indent=2)\n",
    "    \n",
    "    print(\"All validation results saved successfully!\")\n",
    "\n",
    "# Save all results\n",
    "save_all_validation_results()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPREHENSIVE VALIDATION AND TESTING COMPLETED\")\n",
    "print(\"=\"*60)\n",
    "print(\"The following analyses have been performed:\")\n",
    "print(\" Temporal validation with time-based splits\")\n",
    "print(\" Clinical performance metrics evaluation\")\n",
    "print(\" Model interpretability analysis (SHAP)\")\n",
    "print(\" Bias and fairness evaluation\")\n",
    "print(\" Robustness and stability testing\")\n",
    "print(\" Model calibration analysis\")\n",
    "print(\" Comprehensive validation report generation\")\n",
    "print(f\"\\nAll results saved to: {config.RESULTS_PATH}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
